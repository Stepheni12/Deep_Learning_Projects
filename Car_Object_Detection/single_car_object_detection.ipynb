{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Import necessary libraries\nimport numpy as np \nimport pandas as pd\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom typing import Tuple, List, Dict, Optional\nfrom collections import OrderedDict\nfrom os import listdir\nfrom os.path import isfile, join\n\nfrom torchvision import models\nimport torchvision.ops.boxes as bops\nimport torchvision.transforms as T\nfrom torchvision.utils import draw_bounding_boxes\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection.roi_heads import fastrcnn_loss\nfrom torchvision.models.detection.rpn import concat_box_prediction_layers\n\nimport torch\nimport torch.nn as nn\nfrom torch import Tensor\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.utils.data import Dataset, DataLoader\n\n#Function that saves a plot of the validation and training data\ndef save_metric_plot(train_metric, val_metric):\n    x = [i for i in range(len(train_metric))]\n    plt.plot(x,train_metric,label=\"Train Loss\")\n    plt.plot(x,val_metric,label=\"Val Loss\")\n    plt.legend()\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.title(\"Train/Val Loss\")\n    file_name = join(\"/kaggle/working\", \"Train_Val Loss\")\n    plt.savefig(file_name)\n    #plt.clf()\n    print(\"Train/Val loss plot saved.\")\n\n#Function that defines the transformations of the data \ndef get_transform(train):\n    transforms = []\n    transforms.append(T.ToTensor())\n    transforms.append(T.ConvertImageDtype(torch.float))\n    return T.Compose(transforms)\n    \n#Create a class to convert image dataset into PyTorch Dataset class\n#Need to define the three functions listed when creating a pytorch dataset: __init__, __getitem__, and __len__\n\n#Defining a dataset class for your data is actually super efficient with memory as you can create it in a way that\n#you are only reading the current batch images into memory as opposed to the entire dataset. This works best when combined with\n#a pytorch dataloader\n\n#This Dataset class is slightly different than others as it's for object detection and pytorch expects this \"target\" dictionary object that's \n#specifically for detection models, usually the __getitem__ class would be a bit simpler\nclass ObjectDetectionDataset(Dataset):\n\n    def __init__(self, image_files, boxes, transform=None):\n        self.image_files = image_files\n        boxes = np.array(boxes)\n        self.boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        self.transform = transform\n    \n    def __getitem__(self, idx):\n        img_path = self.image_files[idx]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if self.transform:\n            img = self.transform(img)\n            \n        labels = torch.ones((1,), dtype=torch.int64)\n        image_id = torch.tensor([idx])\n        area = (self.boxes[:,2] - self.boxes[:,0]) * (self.boxes[:,3] - self.boxes[:,1])\n        iscrowd = torch.zeros((1,), dtype=torch.int64)\n            \n        target = {}\n        target[\"boxes\"] = self.boxes[idx].unsqueeze(0)\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n            \n        return img, target\n    \n    def __len__(self):\n        return len(self.image_files)\n    \n#The pytorch object detection models are a little quirky and don't provide any loss information when models are in eval mode \n#so this is a pretty simple utility that some kind person on the internet created, thanks jhso from stackoverflow.\n#https://stackoverflow.com/questions/71288513/how-can-i-determine-validation-loss-for-faster-rcnn-pytorch\n\n#The next function is also from jhso so the two of those combined will allow you to obtain the loss functions from your object detection models when \n#evaluating on test data\ndef eval_forward(model, images, targets):\n    # type: (List[Tensor], Optional[List[Dict[str, Tensor]]]) -> Tuple[Dict[str, Tensor], List[Dict[str, Tensor]]]\n    \"\"\"\n    Args:\n        images (list[Tensor]): images to be processed\n        targets (list[Dict[str, Tensor]]): ground-truth boxes present in the image (optional)\n    Returns:\n        result (list[BoxList] or dict[Tensor]): the output from the model.\n            It returns list[BoxList] contains additional fields\n            like `scores`, `labels` and `mask` (for Mask R-CNN models).\n    \"\"\"\n    model.eval()\n\n    original_image_sizes: List[Tuple[int, int]] = []\n    for img in images:\n        val = img.shape[-2:]\n        assert len(val) == 2\n        original_image_sizes.append((val[0], val[1]))\n\n    images, targets = model.transform(images, targets)\n\n    # Check for degenerate boxes\n    # TODO: Move this to a function\n    if targets is not None:\n        for target_idx, target in enumerate(targets):\n            boxes = target[\"boxes\"]\n            degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n            if degenerate_boxes.any():\n                # print the first degenerate box\n                bb_idx = torch.where(degenerate_boxes.any(dim=1))[0][0]\n                degen_bb: List[float] = boxes[bb_idx].tolist()\n                raise ValueError(\n                    \"All bounding boxes should have positive height and width.\"\n                    f\" Found invalid box {degen_bb} for target at index {target_idx}.\"\n                )\n\n    features = model.backbone(images.tensors)\n    if isinstance(features, torch.Tensor):\n        features = OrderedDict([(\"0\", features)])\n    model.rpn.training=True\n    #model.roi_heads.training=True\n\n\n    #####proposals, proposal_losses = model.rpn(images, features, targets)\n    features_rpn = list(features.values())\n    objectness, pred_bbox_deltas = model.rpn.head(features_rpn)\n    anchors = model.rpn.anchor_generator(images, features_rpn)\n\n    num_images = len(anchors)\n    num_anchors_per_level_shape_tensors = [o[0].shape for o in objectness]\n    num_anchors_per_level = [s[0] * s[1] * s[2] for s in num_anchors_per_level_shape_tensors]\n    objectness, pred_bbox_deltas = concat_box_prediction_layers(objectness, pred_bbox_deltas)\n    # apply pred_bbox_deltas to anchors to obtain the decoded proposals\n    # note that we detach the deltas because Faster R-CNN do not backprop through\n    # the proposals\n    proposals = model.rpn.box_coder.decode(pred_bbox_deltas.detach(), anchors)\n    proposals = proposals.view(num_images, -1, 4)\n    proposals, scores = model.rpn.filter_proposals(proposals, objectness, images.image_sizes, num_anchors_per_level)\n\n    proposal_losses = {}\n    assert targets is not None\n    labels, matched_gt_boxes = model.rpn.assign_targets_to_anchors(anchors, targets)\n    regression_targets = model.rpn.box_coder.encode(matched_gt_boxes, anchors)\n    loss_objectness, loss_rpn_box_reg = model.rpn.compute_loss(\n        objectness, pred_bbox_deltas, labels, regression_targets\n    )\n    proposal_losses = {\n        \"loss_objectness\": loss_objectness,\n        \"loss_rpn_box_reg\": loss_rpn_box_reg,\n    }\n\n    #####detections, detector_losses = model.roi_heads(features, proposals, images.image_sizes, targets)\n    image_shapes = images.image_sizes\n    proposals, matched_idxs, labels, regression_targets = model.roi_heads.select_training_samples(proposals, targets)\n    box_features = model.roi_heads.box_roi_pool(features, proposals, image_shapes)\n    box_features = model.roi_heads.box_head(box_features)\n    class_logits, box_regression = model.roi_heads.box_predictor(box_features)\n\n    result: List[Dict[str, torch.Tensor]] = []\n    detector_losses = {}\n    loss_classifier, loss_box_reg = fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n    detector_losses = {\"loss_classifier\": loss_classifier, \"loss_box_reg\": loss_box_reg}\n    boxes, scores, labels = model.roi_heads.postprocess_detections(class_logits, box_regression, proposals, image_shapes)\n    num_images = len(boxes)\n    for i in range(num_images):\n        result.append(\n            {\n                \"boxes\": boxes[i],\n                \"labels\": labels[i],\n                \"scores\": scores[i],\n            }\n        )\n    detections = result\n    detections = model.transform.postprocess(detections, images.image_sizes, original_image_sizes)  # type: ignore[operator]\n    model.rpn.training=False\n    model.roi_heads.training=False\n    losses = {}\n    losses.update(detector_losses)\n    losses.update(proposal_losses)\n    return losses, detections\n\ndef evaluate_loss(model, data_loader, device):\n    val_loss = 0\n    with torch.no_grad():\n        for images, targets in data_loader:\n          images = list(image.to(device) for image in images)\n          targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n          loss_dict, detections = eval_forward(model, images, targets)\n         \n          losses = sum(loss for loss in loss_dict.values())\n\n          val_loss += losses\n          \n    validation_loss = val_loss/ len(data_loader)    \n    return validation_loss\n\n#Honestly not entirely sure what this does but it's required so that the batches work with the object detection model. I think it may have to do \n#with pulling from the dataloader as a dictionary which is what the model wants as input \ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\n### Preparing input data ###\n\n#Read bounding box labels in pandas dataframe\nlabels_df = pd.read_csv(\"../input/car-object-detection/data/train_solution_bounding_boxes (1).csv\")\nlabels_lst = []\n\n#For each row of the dataframe store the bounding box coordinates and add this list to a global list holding all the bb labels\nfor i in range(labels_df.shape[0]):\n    tmp = labels_df.iloc[i,1:].tolist()\n    labels_lst.append(tmp)\n    \nlabels = np.array(labels_lst, dtype=np.float32)\n\n#Not every image has an accompanying label so this goes through the list of all labels \n#and pulls the file names of all images that have labels\npath = \"../input/car-object-detection/data/training_images\"\nall_files = [join(path,f) for f in labels_df.iloc[:,0]]\n\n#Split train set into train and validation set\ntrain_files, val_files, train_labels, val_labels = train_test_split(all_files[:-59], labels[:-59], random_state=1, train_size = .80)\n\ntest_files = all_files[-59:]\ntest_labels = labels[-59:]\n    \n#Instantiate train and val datasets and dataloaders\ntrain_dataset = ObjectDetectionDataset(train_files, train_labels, transform=get_transform(train=True))\nval_dataset = ObjectDetectionDataset(val_files, val_labels, transform=get_transform(train=False))\ntest_dataset = ObjectDetectionDataset(test_files, test_labels, transform=get_transform(train=False))\ntrain_dataloader = DataLoader(train_dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)\nval_dataloader = DataLoader(val_dataset, batch_size=5, shuffle=False, collate_fn=collate_fn)\n    \n#Determine if running on GPU or CPU\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n#Load \nmodel_conv = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n\n#One class for the object (car) +  one class for the background\nnum_classes = 2 \n\n#Not entirely sure of exactly how all this works, but since we are transfer learning this step is to essentially create a new head in order \n#to adjust for our specific dataset\nin_features = model_conv.roi_heads.box_predictor.cls_score.in_features\nmodel_conv.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel_conv = model_conv.to(device)\n\nparams = [p for p in model_conv.parameters() if p.requires_grad]\n\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\nlr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n\ndef train_model(model, opt, scheduler, train_loader, val_loader, epochs=100):\n    print(\"Beginning Training\")\n    best_loss = np.inf\n    all_train_losses = []\n    all_val_losses = []\n    \n    epochs_without_improve = 0\n    early_stop = False\n    patience = 5 # If no improvement in validation after X straight epochs, early stop activates\n    \n    for epoch in range(epochs):\n        train_losses = []\n        train_losses_dict = []\n        model.train()\n        train_progress = tqdm(train_loader, desc=\"Epoch [%s/%s]:\" % (epoch, epochs))\n        \n        for imgs, targets in train_progress:\n            imgs = list(image.to(device) for image in imgs)\n            targets = [{k: torch.tensor(v).to(device) for k, v in t.items()} for t in targets]\n            opt.zero_grad()\n            loss_dict = model(imgs, targets) #supposedly model computes loss automatically, not sure how\n            losses = sum(loss for loss in loss_dict.values())\n            loss_dict_append = {k: v.item() for k, v in loss_dict.items()}\n            loss_value = losses.item()\n        \n            train_losses.append(loss_value)\n            train_losses_dict.append(loss_dict_append)\n            #Just a small note for my future self reading this, I spent a couple days debugging why the model wasn't training and it was because at\n            #some point I accidentally deleted the line calling backward below. I had a good laugh once I realized.\n            losses.backward() \n            opt.step()\n            train_progress.set_description(\"Epoch [%s/%s] Loss: [%.4f]\" % (epoch, epochs, loss_value))\n        \n        scheduler.step()\n        train_losses_dict = pd.DataFrame(train_losses_dict)\n        print(\"Epoch {}, lr: {:.6f}, loss: {:.6f}, loss_classifier: {:.6f}, loss_box: {:.6f}, loss_rpn_box: {:.6f}, loss_object: {:.6f}\".format(\n        epoch, optimizer.param_groups[0]['lr'], np.mean(train_losses),\n            train_losses_dict['loss_classifier'].mean(),\n            train_losses_dict['loss_box_reg'].mean(),\n            train_losses_dict['loss_rpn_box_reg'].mean(),\n            train_losses_dict['loss_objectness'].mean()\n        ))\n        \n        all_train_losses.append(np.mean(train_losses))\n          \n        #Validate\n        validation_loss  = evaluate_loss(model, val_loader, device=device)\n        all_val_losses.append(validation_loss.cpu().numpy())\n        \n        if validation_loss < best_loss:\n            print(\"Loss [%.4f] improved from [%.4f].\" % (validation_loss, best_loss))\n            print(\"Saving model to current directory.\")\n            torch.save(model.state_dict(), \"/kaggle/working/best_model\")\n            best_loss = validation_loss\n            epochs_without_improve = 0\n        else:\n            epochs_without_improve += 1\n            print(\"Loss [%.4f] did not improve from [%.4f].\" % (validation_loss, best_loss))\n        print('-'*20)\n        \n        #Check for early stopping\n        if epochs_without_improve == patience:\n            print(\"Early Stopping because there was no improvement in \" + str(patience) + \" straight epochs.\")\n            early_stop = True\n            save_metric_plot(all_train_losses, all_val_losses)\n            break\n    if not early_stop:\n        save_metric_plot(all_train_losses, all_val_losses)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-14T22:06:00.878529Z","iopub.execute_input":"2022-09-14T22:06:00.878864Z","iopub.status.idle":"2022-09-14T22:06:24.814823Z","shell.execute_reply.started":"2022-09-14T22:06:00.878787Z","shell.execute_reply":"2022-09-14T22:06:24.813806Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"train_model(model_conv, optimizer, lr_scheduler, train_dataloader, val_dataloader)","metadata":{"execution":{"iopub.status.busy":"2022-09-14T22:09:49.468223Z","iopub.execute_input":"2022-09-14T22:09:49.468739Z","iopub.status.idle":"2022-09-14T22:27:52.244321Z","shell.execute_reply.started":"2022-09-14T22:09:49.468694Z","shell.execute_reply":"2022-09-14T22:27:52.243379Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#Display test images with predictions\nclasses = [\"ignore\", \"car\"]\nmodel_conv.eval()\ntorch.cuda.empty_cache()\n\ntrans = T.Compose([T.ToTensor()])\ntoPIL = T.ToPILImage()\n\ntst_img, _ = test_dataset[0]\n\nwith torch.no_grad():\n    output = model_conv([tst_img.to(device)])\n    pred = output[0]\n    \n#Only display predictions with scores over a certain threshold\nfig = plt.figure(figsize=(14, 10))\nplt.imshow(draw_bounding_boxes(img_int,\n    pred['boxes'][pred['scores'] > 0.800],\n    [classes[i] for i in pred['labels'][pred['scores'] > 0.800].tolist()], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T23:12:37.525939Z","iopub.execute_input":"2022-09-14T23:12:37.526371Z","iopub.status.idle":"2022-09-14T23:12:38.094132Z","shell.execute_reply.started":"2022-09-14T23:12:37.526334Z","shell.execute_reply":"2022-09-14T23:12:38.091596Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"#Display image from training dataset\nsample = test_dataset[0]\nimg_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n\n#print(sample)\n\nplt.imshow(draw_bounding_boxes(\n    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n).permute(1, 2, 0))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T23:12:22.224348Z","iopub.execute_input":"2022-09-14T23:12:22.224746Z","iopub.status.idle":"2022-09-14T23:12:22.504584Z","shell.execute_reply.started":"2022-09-14T23:12:22.224715Z","shell.execute_reply":"2022-09-14T23:12:22.503664Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"#Evaluate test dataset\ndef eval_model(model, test_dataset):\n    model.eval()\n    torch.cuda.empty_cache()\n    results = []\n\n    for tst_im in test_dataset:\n        ground_truth = tst_im[1]['boxes']\n        ground_truth = ground_truth.to(device)\n\n        with torch.no_grad():\n            output = model([tst_im[0].to(device)])\n            pred = output[0]\n\n\n        valid_boxes = pred['boxes'][pred['scores'] > 0.800]\n\n        curr_results = []\n        for box in valid_boxes:\n            box = box.unsqueeze(0)\n            tmp = bops.box_iou(box, ground_truth)\n            curr_results.append(tmp[0][0])\n\n        results.append(max(curr_results).cpu().numpy())\n\n    print(\"Average IoU: \", np.mean(results))","metadata":{"execution":{"iopub.status.busy":"2022-09-14T23:30:38.890548Z","iopub.execute_input":"2022-09-14T23:30:38.890900Z","iopub.status.idle":"2022-09-14T23:30:43.499176Z","shell.execute_reply.started":"2022-09-14T23:30:38.890872Z","shell.execute_reply":"2022-09-14T23:30:43.498086Z"},"trusted":true},"execution_count":48,"outputs":[]}]}