{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9158badd",
   "metadata": {},
   "source": [
    "# Neural Image Caption\n",
    "Original paper: https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "Not exactly the same but implementing something similar with more up to date tools\n",
    "\n",
    "Basic idea: Input Image -> CNN -> Image Embedding -> LSTM -> Image Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d0d7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileViTImageProcessor, MobileViTForImageClassification, AutoModel, AutoConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Fixes matplotlib crashing jupyter kernel issue (should find out what this actually does, something to do with OpenMP?)\n",
    "import os\n",
    "# os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff09a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read images into array\n",
    "\n",
    "# Image paths\n",
    "image_paths = [\"cat1.png\", \"cat2.jpg\", \"river.jpg\"]\n",
    "\n",
    "# Open the images\n",
    "images = [Image.open(path).resize((256,256)) for path in image_paths]\n",
    "\n",
    "# Convert the PIL images to NumPy arrays\n",
    "image_arrays = [np.array(image) for image in images]\n",
    "\n",
    "# Combine the arrays into a single array (stack vertically)\n",
    "combined_array = np.stack(image_arrays, axis=0)    \n",
    "print(combined_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fd4bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 320])\n"
     ]
    }
   ],
   "source": [
    "# Some unrelated notes on hugging face that's good to remember\n",
    "#\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "#\n",
    "# MobileViTForImageClassification vs. AutoModel\n",
    "# For our use case we need to obtain image embeddings so using AutoModel makes more sense as it outputs the dense \n",
    "# representations of the images and not the logits, which are what MobileViTForImageClassification would have provided\n",
    "#\n",
    "# This would be for actual image classification:\n",
    "#\n",
    "# model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "# outputs = model(**inputs, output_hidden_states=True)\n",
    "# logits = outputs.logits\n",
    "#\n",
    "# # model predicts one of the 1000 ImageNet classes\n",
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
    "\n",
    "# Pulling pre-trained image classification model from hugging face\n",
    "feature_extractor = MobileViTImageProcessor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "model = AutoModel.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "\n",
    "# Preprocess batch of images\n",
    "# Images can be PIL image, numpy array, or torch tensor individually or in list form\n",
    "inputs = feature_extractor(images=combined_array, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "out = outputs.pooler_output # backup plan: last_hidden_state for embeddings instead\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb7ba27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat1 vs. River similarity: tensor(-0.0756)\n",
      "Cat1 vs. Cat2 similarity: tensor(0.5102)\n"
     ]
    }
   ],
   "source": [
    "# Testing embeddings\n",
    "with torch.no_grad():\n",
    "    sim1 = torch.nn.functional.cosine_similarity(out[0], out[2], dim=0)\n",
    "    sim2 = torch.nn.functional.cosine_similarity(out[0], out[1], dim=0)\n",
    "    print(\"Cat1 vs. River similarity:\", sim1)\n",
    "    print(\"Cat1 vs. Cat2 similarity:\", sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c990ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoderCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoderCNN, self).__init__()\n",
    "        # Load pre-trained model\n",
    "        self.model = AutoModel.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "        \n",
    "        # Freeze layers of pre-trained model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 320 is the size the pooler_output that I'm using as the last layer of the pre-trained.\n",
    "        # Passed through this liner layer to get to the same size as the word embeddings so I can pass it to the LSTM.\n",
    "        # Future: Maybe look into using different layer from pre-trained model as the output\n",
    "        self.linear = torch.nn.Linear(320, 384)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        outputs = self.model(images)\n",
    "        \n",
    "        # Pull image embeddings\n",
    "        embeddings = outputs.pooler_output\n",
    "        \n",
    "        # Pass image embeddings through linear layer to reach desired size of LSTM input\n",
    "        image_embeddings = self.linear(embeddings)\n",
    "        \n",
    "        return image_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0f2d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/build_vocab.py\n",
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(dataframe, threshold=4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    counter = Counter()\n",
    "\n",
    "    for caption in dataframe['caption']:\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54dccaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class FlickrDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_dir, dataframe, vocab, image_processor):\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.image_processor = image_processor\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_file = row.iloc[0]\n",
    "        caption = row.iloc[1]\n",
    "        \n",
    "        # Convert caption (string) to word ids\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.tensor(caption, dtype=torch.long)\n",
    "        \n",
    "        raw_image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
    "        processed_image = self.image_processor(images=raw_image, return_tensors=\"pt\")\n",
    "        image = processed_image.pixel_values.squeeze(0)\n",
    "        \n",
    "        return image, target\n",
    "    \n",
    "    def viewImage(self, idx):\n",
    "    \n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_file = row.iloc[0]\n",
    "        caption = row.iloc[1]\n",
    "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
    "            \n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90c1a08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f848f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 562 ms\n",
      "Wall time: 485 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate embeddings for the vocab\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-V2')\n",
    "embeddings_lst = []\n",
    "\n",
    "def embedding(texts):\n",
    "    embs = sent_model.encode(texts)\n",
    "    return embs\n",
    "\n",
    "# for i in range(len(vocab)):\n",
    "#     word = vocab.idx2word[i]\n",
    "#     embeddings_lst.append(embedding(word))\n",
    "#     if i % 500 == 0:\n",
    "#         print(i)\n",
    "        \n",
    "# np_arr = np.array(embeddings_lst)\n",
    "# final_embeddings = torch.Tensor(np_arr)\n",
    "# print(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d631cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_embeddings = torch.load(\"embeddings_table.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d37238bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptionDecoderRNN(torch.nn.Module):\n",
    "    def __init__(self, word_embeddings, hidden_size, embedding_size, vocab_size, max_seq_length=25):\n",
    "        super(CaptionDecoderRNN, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embed = torch.nn.Embedding.from_pretrained(word_embeddings, freeze=True) # Future: Try unfreezing?\n",
    "        self.lstm = torch.nn.LSTM(embedding_size, hidden_size)\n",
    "        self.linear = torch.nn.Linear(hidden_size, vocab_size)\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def forward(self, image_embeddings, captions, lengths):\n",
    "        embeddings = self.embed(captions)\n",
    "        embeddings = torch.cat((image_embeddings.unsqueeze(1), embeddings), 1)\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "        hiddens, _ = self.lstm(packed)\n",
    "        outputs = self.linear(hiddens[0])\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        for i in range(self.max_seq_length):\n",
    "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c33063ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(image_dir, dataframe, vocab, image_processor, batch_size=128):\n",
    "    flickr = FlickrDataset(image_dir, dataframe, vocab, image_processor)\n",
    "\n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=flickr, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f468304e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 5043563\n",
      "torch.Size([256, 23])\n",
      "CPU times: total: 14.5 s\n",
      "Wall time: 8.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# FlickrDataset test\n",
    "torch.manual_seed(9856)\n",
    "df = pd.read_csv(\"captions.txt\")\n",
    "df['caption'] = df['caption'].str.lower()\n",
    "df['caption'] = df['caption'].str.replace(r\"[^a-zA-Z0-9-' ]\", '', regex=True)\n",
    "\n",
    "# transform = v2.Compose([ \n",
    "#     v2.PILToTensor() \n",
    "# ])\n",
    "\n",
    "# vocab = build_vocab(df, threshold=4)\n",
    "# vocab_path = \"./vocab.pkl\"\n",
    "# with open(vocab_path, 'wb') as f:\n",
    "#     pickle.dump(vocab, f)\n",
    "# print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "# print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))\n",
    "\n",
    "# Define embedding model\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-V2')\n",
    "\n",
    "# Define image processor\n",
    "ImageProcessor = MobileViTImageProcessor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "\n",
    "with open(\"./vocab.pkl\", 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "data_loader = create_dataloader(\"./Images\", df, vocab, ImageProcessor, batch_size=256)\n",
    "\n",
    "# Define models\n",
    "encoder = ImageEncoderCNN()\n",
    "decoder = CaptionDecoderRNN(final_embeddings, 512, final_embeddings.shape[1], len(vocab))\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "all_params = list(decoder.parameters()) + list(encoder.linear.parameters())\n",
    "print(\"Parameters:\",sum(p.nelement() for p in all_params))\n",
    "opt = torch.optim.Adam(all_params, lr=0.0003)\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, captions, lengths) in enumerate(data_loader):\n",
    "        print(captions.shape)\n",
    "        break\n",
    "#         targets = torch.nn.utils.rnn.pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "        \n",
    "#         image_features = encoder(images)\n",
    "#         logits = decoder(image_features, captions, lengths)\n",
    "#         loss = torch.nn.functional.cross_entropy(logits, targets)\n",
    "#         decoder.zero_grad()\n",
    "#         encoder.zero_grad()\n",
    "#         loss.backward()\n",
    "#         opt.step()\n",
    "        \n",
    "#         if i % 25 == 0:\n",
    "#             print(loss.item())\n",
    "#             torch.save(decoder.state_dict(), './decoder_{}_{}.ckpt'.format(epoch, i))\n",
    "#             torch.save(encoder.state_dict(), './encoder_{}_{}.ckpt'.format(epoch, i))\n",
    "#         break\n",
    "            \n",
    "# # Save the model\n",
    "# torch.save(decoder.state_dict(), './decoder_final.ckpt')\n",
    "# torch.save(encoder.state_dict(), './encoder_final.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
