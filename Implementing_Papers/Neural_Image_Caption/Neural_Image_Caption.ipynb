{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9158badd",
   "metadata": {},
   "source": [
    "# Neural Image Caption\n",
    "Original paper: https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "Not exactly the same but implementing something similar with more up to date tools\n",
    "\n",
    "Basic idea: Input Image -> CNN -> Image Embedding -> LSTM -> Image Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7d0d7e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\datapoints\\__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torchvision\\transforms\\v2\\__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from transformers import MobileViTImageProcessor, MobileViTForImageClassification, AutoModel\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Fixes matplotlib crashing jupyter kernel issue (should find out what this actually does, something to do with OpenMP?)\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff09a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read images into array\n",
    "\n",
    "# Image paths\n",
    "image_paths = [\"cat1.png\", \"cat2.jpg\", \"river.jpg\"]\n",
    "\n",
    "# Open the images\n",
    "images = [Image.open(path).resize((256,256)) for path in image_paths]\n",
    "\n",
    "# Convert the PIL images to NumPy arrays\n",
    "image_arrays = [np.array(image) for image in images]\n",
    "\n",
    "# Combine the arrays into a single array (stack vertically)\n",
    "combined_array = np.stack(image_arrays, axis=0)    \n",
    "print(combined_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fd4bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 320])\n"
     ]
    }
   ],
   "source": [
    "# Some unrelated notes on hugging face that's good to remember\n",
    "#\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "#\n",
    "# MobileViTForImageClassification vs. AutoModel\n",
    "# For our use case we need to obtain image embeddings so using AutoModel makes more sense as it outputs the dense \n",
    "# representations of the images and not the logits, which are what MobileViTForImageClassification would have provided\n",
    "#\n",
    "# This would be for actual image classification:\n",
    "#\n",
    "# model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "# outputs = model(**inputs, output_hidden_states=True)\n",
    "# logits = outputs.logits\n",
    "#\n",
    "# # model predicts one of the 1000 ImageNet classes\n",
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
    "\n",
    "# Pulling pre-trained image classification model from hugging face\n",
    "feature_extractor = MobileViTImageProcessor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "model = AutoModel.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "\n",
    "# Preprocess batch of images\n",
    "# Images can be PIL image, numpy array, or torch tensor individually or in list form\n",
    "inputs = feature_extractor(images=combined_array, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "out = outputs.pooler_output # backup plan: last_hidden_state for embeddings instead\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7ba27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat1 vs. River similarity: tensor(-0.0756)\n",
      "Cat1 vs. Cat2 similarity: tensor(0.5102)\n"
     ]
    }
   ],
   "source": [
    "# Testing embeddings\n",
    "with torch.no_grad():\n",
    "    sim1 = torch.nn.functional.cosine_similarity(out[0], out[2], dim=0)\n",
    "    sim2 = torch.nn.functional.cosine_similarity(out[0], out[1], dim=0)\n",
    "    print(\"Cat1 vs. River similarity:\", sim1)\n",
    "    print(\"Cat1 vs. Cat2 similarity:\", sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c990ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        self.model = AutoModel.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "    \n",
    "    def forward(self, images):\n",
    "        output = self.model(**images)\n",
    "        \n",
    "        # Pull embeddings\n",
    "        embeddings = output.pooler_output\n",
    "        \n",
    "        return embeddings\n",
    "    \n",
    "encoder = ImageEncoder()\n",
    "encoder.train()\n",
    "features = encoder(inputs)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a786a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing input data ###\n",
    "df = pd.read_csv(\"captions.txt\")\n",
    "df['caption'] = df['caption'].str.lower()\n",
    "df['caption'] = df['caption'].str.replace(r\"[^a-zA-Z0-9-' ]\", '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54dccaa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 320])\n"
     ]
    }
   ],
   "source": [
    "# Create dataset class\n",
    "class FlickrDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_dir, dataframe, transform):\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.dataframe = dataframe\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_file = row[0]\n",
    "        caption = row[1]\n",
    "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "            \n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a7dec4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For later\n",
    "transform = v2.Compose([ \n",
    "    v2.PILToTensor() \n",
    "])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
