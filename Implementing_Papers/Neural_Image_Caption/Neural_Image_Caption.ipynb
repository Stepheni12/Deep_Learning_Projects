{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9158badd",
   "metadata": {},
   "source": [
    "# Neural Image Caption\n",
    "Original paper: https://arxiv.org/pdf/1411.4555.pdf\n",
    "\n",
    "Not exactly the same but implementing something similar with more up to date tools\n",
    "\n",
    "Basic idea: Input Image -> CNN -> Image Embedding -> LSTM -> Image Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "7d0d7e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MobileViTImageProcessor, MobileViTForImageClassification, AutoModel, AutoConfig\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from torchvision.transforms import v2\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Fixes matplotlib crashing jupyter kernel issue (should find out what this actually does, something to do with OpenMP?)\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ff09a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "# Read images into array\n",
    "\n",
    "# Image paths\n",
    "image_paths = [\"cat1.png\", \"cat2.jpg\", \"river.jpg\"]\n",
    "\n",
    "# Open the images\n",
    "images = [Image.open(path).resize((256,256)) for path in image_paths]\n",
    "\n",
    "# Convert the PIL images to NumPy arrays\n",
    "image_arrays = [np.array(image) for image in images]\n",
    "\n",
    "# Combine the arrays into a single array (stack vertically)\n",
    "combined_array = np.stack(image_arrays, axis=0)    \n",
    "print(combined_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7fd4bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 320])\n"
     ]
    }
   ],
   "source": [
    "# Some unrelated notes on hugging face that's good to remember\n",
    "#\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "#\n",
    "# MobileViTForImageClassification vs. AutoModel\n",
    "# For our use case we need to obtain image embeddings so using AutoModel makes more sense as it outputs the dense \n",
    "# representations of the images and not the logits, which are what MobileViTForImageClassification would have provided\n",
    "#\n",
    "# This would be for actual image classification:\n",
    "#\n",
    "# model = MobileViTForImageClassification.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "# outputs = model(**inputs, output_hidden_states=True)\n",
    "# logits = outputs.logits\n",
    "#\n",
    "# # model predicts one of the 1000 ImageNet classes\n",
    "# predicted_class_idx = logits.argmax(-1).item()\n",
    "# print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n",
    "\n",
    "# Pulling pre-trained image classification model from hugging face\n",
    "feature_extractor = MobileViTImageProcessor.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "model = AutoModel.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "\n",
    "# Preprocess batch of images\n",
    "# Images can be PIL image, numpy array, or torch tensor individually or in list form\n",
    "inputs = feature_extractor(images=combined_array, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "out = outputs.pooler_output # backup plan: last_hidden_state for embeddings instead\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb7ba27b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cat1 vs. River similarity: tensor(-0.0756)\n",
      "Cat1 vs. Cat2 similarity: tensor(0.5102)\n"
     ]
    }
   ],
   "source": [
    "# Testing embeddings\n",
    "with torch.no_grad():\n",
    "    sim1 = torch.nn.functional.cosine_similarity(out[0], out[2], dim=0)\n",
    "    sim2 = torch.nn.functional.cosine_similarity(out[0], out[1], dim=0)\n",
    "    print(\"Cat1 vs. River similarity:\", sim1)\n",
    "    print(\"Cat1 vs. Cat2 similarity:\", sim2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0c990ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        # Load pre-trained model\n",
    "        self.model = AutoModel.from_pretrained(\"apple/mobilevit-xx-small\")\n",
    "        \n",
    "        # Freeze layers of pre-trained model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # 320 is the size the pooler_output that I'm using as the last layer of the pre-trained.\n",
    "        # Passed through this liner layer to get to the same size as the word embeddings so I can pass it to the LSTM.\n",
    "        # Future: Maybe look into using different layer from pre-trained model as the output\n",
    "        self.linear = torch.nn.Linear(320, 384)\n",
    "        \n",
    "    def forward(self, images):\n",
    "        output = self.model(**images)\n",
    "        \n",
    "        # Pull image embeddings\n",
    "        embeddings = output.pooler_output\n",
    "        \n",
    "        # Pass image embeddings through linear layer to reach desired size of LSTM input\n",
    "        image_embeddings = self.linear(embeddings)\n",
    "        \n",
    "        return image_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bd9df5e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 384])\n"
     ]
    }
   ],
   "source": [
    "# ImageEncoder test\n",
    "encoder = ImageEncoder()\n",
    "encoder.train()\n",
    "features = encoder(inputs)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1a0f2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 3435\n",
      "Saved the vocabulary wrapper to './vocab.pkl'\n"
     ]
    }
   ],
   "source": [
    "# https://github.com/yunjey/pytorch-tutorial/blob/master/tutorials/03-advanced/image_captioning/build_vocab.py\n",
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(dataframe, threshold=4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    counter = Counter()\n",
    "\n",
    "    for caption in dataframe['caption']:\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "vocab = build_vocab(df, threshold=4)\n",
    "vocab_path = \"./vocab.pkl\"\n",
    "with open(vocab_path, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "print(\"Total vocabulary size: {}\".format(len(vocab)))\n",
    "print(\"Saved the vocabulary wrapper to '{}'\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "54dccaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class FlickrDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, image_dir, dataframe, vocab, transform):\n",
    "        \n",
    "        self.image_dir = image_dir\n",
    "        self.dataframe = dataframe\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_file = row.iloc[0]\n",
    "        caption = row.iloc[1]\n",
    "        \n",
    "        # Convert caption (string) to word ids\n",
    "        tokens = nltk.tokenize.word_tokenize(caption)\n",
    "        caption = []\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        \n",
    "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
    "        image = self.transform(image)\n",
    "            \n",
    "        return image, target\n",
    "    \n",
    "    def viewImage(self, idx):\n",
    "    \n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_file = row.iloc[0]\n",
    "        caption = row.iloc[1]\n",
    "        image = Image.open(os.path.join(self.image_dir, image_file)).convert('RGB')\n",
    "            \n",
    "        return image, caption\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a7dec4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FlickrDataset test\n",
    "df = pd.read_csv(\"captions.txt\")\n",
    "df['caption'] = df['caption'].str.lower()\n",
    "df['caption'] = df['caption'].str.replace(r\"[^a-zA-Z0-9-' ]\", '', regex=True)\n",
    "\n",
    "transform = v2.Compose([ \n",
    "    v2.PILToTensor() \n",
    "])\n",
    "\n",
    "# Define embedding model\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-V2')\n",
    "\n",
    "with open(\"./vocab.pkl\", 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "data = FlickrDataset(\"./images\", df, vocab, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b88ed8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.,  4.,  5.,  6.,  4.,  7.,  8.,  9., 10., 11.,  4., 12., 13., 14.,\n",
      "         6., 15.,  3., 16.,  2.])\n"
     ]
    }
   ],
   "source": [
    "im, cap = data[0]\n",
    "print(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6f848f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "CPU times: total: 1min 30s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Generate embeddings for vocab\n",
    "sent_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-V2')\n",
    "embeddings_lst = []\n",
    "\n",
    "def embedding(texts):\n",
    "    embs = sent_model.encode(texts)\n",
    "    return embs\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab.idx2word[i]\n",
    "    embeddings_lst.append(embedding(word))\n",
    "    if i % 500 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "1800f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3435, 384])\n"
     ]
    }
   ],
   "source": [
    "np_arr = np.array(embeddings_lst)\n",
    "final_embeddings = torch.Tensor(np_arr)\n",
    "print(final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d631cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(final_embeddings, \"embeddings_table.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37238bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(self, word_embeddings):\n",
    "        super(Decoder, self).__init__()\n",
    "        # Load pre-trained embeddings\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(word_embeddings, freeze=True) # Future: Try unfreezing?\n",
    "        \n",
    "        \n",
    "    def forward(self, images):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
