{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421f8ea1",
   "metadata": {},
   "source": [
    "## Attention Is All You Need\n",
    "\n",
    "Original paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Implementing a transformer kind of from scratch using numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "29c5a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3f667",
   "metadata": {},
   "source": [
    "Need to implement:\n",
    "- [x] Scaled dot-product attention\n",
    "- [x] Multi-head attention\n",
    "- [x] Positional encoding\n",
    "- [x] Layer normalization\n",
    "- [x] Position-wise feed forward\n",
    "- [x] Embeddings\n",
    "- [x] Encoder layer (combination of some of the above)\n",
    "- [x] Encoder (stack of encoder layers)\n",
    "- [x] Multi-head cross attention\n",
    "- [x] Decoder layer\n",
    "- [x] Decoder\n",
    "- [x] Transformer (combining encoder and decoder, plus some additional stuff)\n",
    "- [ ] Weight init\n",
    "- [ ] Optimization\n",
    "- [ ] Preprocess, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "1ed01d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    numerator = q @ torch.transpose(k, -2, -1) # May have to fix this transpose\n",
    "    if mask is not None:\n",
    "        numerator = numerator.permute(1, 0, 2, 3) + mask\n",
    "        numerator = numerator.permute(1, 0, 2, 3)\n",
    "    denominator = math.sqrt(k.shape[-1])\n",
    "    attn = F.softmax((numerator/denominator), dim=-1, dtype=torch.float32)\n",
    "    result = attn @ v\n",
    "    return result, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "id": "0fbac151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // heads # Embed dim must be divisible by heads\n",
    "        self.q_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.k_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.v_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.linear_out = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_length, _ = q.size()\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        q, k, v = [x.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1,2) for x in [q,k,v]]\n",
    "        values, attn = scaled_dot_product_attention(q, k, v, mask)\n",
    "        x = values.transpose(1,2).reshape(batch_size, seq_length, self.heads * self.head_dim)\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "id": "95b85eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn((30,50,512))\n",
    "\n",
    "mh = MultiHeadAttention(8, 512)\n",
    "res = mh(test, test, test)\n",
    "print(res.shape)\n",
    "\n",
    "# mh_torch = nn.MultiheadAttention(512, 8, bias=False, batch_first=True)\n",
    "# res1 = mh_torch(test, test, test)\n",
    "# print(res1[0].shape)\n",
    "\n",
    "# Check if tensors equal within threshold\n",
    "#torch.all(torch.lt(torch.abs(torch.add(res, -res1[0])), 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "id": "84ccc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example to visualize why you need this: x.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1,2)\n",
    "# as opposed to just reshaping to that desired shape only using view.\n",
    "ex_q = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "ex_k = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "ex_v = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "r = ex_q.view(2,3,5,6)\n",
    "t = ex_q.view(2,5,3,6).transpose(1,2)\n",
    "\n",
    "# Toy example: 2 batches with a sequence length of 5 and an embedding of size 18.\n",
    "# Keep in mind, ex_q is an example of what q would look like. If you print out ex_q, r, t. You can see that r simply\n",
    "# goes across row by row of ex_q dividing the data amongst the \"heads\" completely incorrectly as it's taking some info from \n",
    "# the first input sequence and then it carries over into the second input sequence, so it's clearly wrong which is why\n",
    "# you need to used both the view and transpose in order to move the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "id": "0fe7ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "id": "30e046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, drop_prob=0.1): # Max seq length is set to 50\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Calculate denominator, it's the same for even and odd dimensions so you can reuse it\n",
    "        evens = torch.arange(0, self.d_model, 2).float()\n",
    "        denom = torch.pow(10000, evens/self.d_model)\n",
    "        \n",
    "        # Calculate positional encodings\n",
    "        self.pe = torch.zeros(self.max_seq_len, self.d_model)\n",
    "        positions = torch.arange(0, self.max_seq_len).float().reshape(self.max_seq_len, 1)\n",
    "        \n",
    "        self.pe[:, 0::2] = torch.sin(positions / denom)\n",
    "        self.pe[:, 1::2] = torch.cos(positions / denom)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "id": "17d3007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameter_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameter_shape = parameter_shape\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Define layer norm learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameter_shape))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # The layer norm is computed based on each matrix of the batch, not across the batch.\n",
    "        mean = inputs.mean(-1, keepdim=True)\n",
    "        std = inputs.std(-1, keepdim=True)\n",
    "        \n",
    "        norm = (self.gamma * ((inputs - mean) / (std + self.eps))) + self.beta\n",
    "        \n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "id": "3a3db39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "id": "b3ee2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x -> Multi-Head Attention -> LayerNorm(residual + x) -> PWFeedForward -> LayerNorm(residual + x)\n",
    "#\n",
    "# MultiHeadAttention: heads, d_model\n",
    "# LayerNormalization: parameter_shape, eps=1e-5\n",
    "# PositionWiseFeedForward: d_model, hidden, drop_prob=0.1\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm1 = LayerNormalization(self.d_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.pwff = PositionWiseFeedForward(self.d_model, self.hidden, self.drop_prob)\n",
    "        self.norm2 = LayerNormalization(self.d_model) # Might have to change this\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attn(x, x, x, mask=mask)\n",
    "        x = self.norm1(residual_x + x)\n",
    "        x = self.drop1(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.pwff(x)\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "id": "bfb9fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.mask_attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm1 = LayerNormalization(self.d_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.cross_attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm2 = LayerNormalization(self.d_model)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        self.pwff = PositionWiseFeedForward(self.d_model, self.hidden, self.drop_prob)\n",
    "        self.norm3 = LayerNormalization(self.d_model) # Might have to change this\n",
    "        self.drop3 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, y, self_mask, cross_mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.mask_attn(x, x, x, mask=self_mask)\n",
    "        x = self.norm1(residual_x + x)\n",
    "        x = self.drop1(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.cross_attn(x, y, y, mask=cross_mask) # FINISH THIS \n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.pwff(x)\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "id": "3d14c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            out = module(x, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "id": "0688ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_mask, cross_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            out = module(x, y, self_mask, cross_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "id": "ad22ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(heads, d_model, hidden) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.layers(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "7521f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(heads, d_model, hidden) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, y, self_mask, cross_mask):\n",
    "        x = self.layers(x, y, self_mask, cross_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "2cf9779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_sequence_length, src_vocab_size, tgt_vocab_size,\n",
    "                 num_layers, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.src_embed = Embeddings(src_vocab_size, d_model)\n",
    "        self.tgt_embed = Embeddings(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.enc_pe = PositionalEncoding(d_model, max_sequence_length, drop_prob)\n",
    "        self.dec_pe = PositionalEncoding(d_model, max_sequence_length, drop_prob)\n",
    "        \n",
    "        self.encoder = Encoder(heads, d_model, hidden, num_layers)\n",
    "        self.decoder = Decoder(heads, d_model, hidden, num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, tgt, enc_self_mask, dec_self_mask, dec_cross_mask):\n",
    "        x = self.src_embed(src)\n",
    "        y = self.tgt_embed(tgt)\n",
    "        \n",
    "        x = self.enc_pe(x)\n",
    "        y = self.dec_pe(y)\n",
    "        \n",
    "        enc = self.encoder(x, enc_self_mask)\n",
    "        dec = self.decoder(y, enc, dec_self_mask, dec_cross_mask)\n",
    "        \n",
    "        out = self.linear(dec)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "2998e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import io\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(file, tokenizer, threshold=4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    with io.open(file, 'r', encoding='utf-8') as file:\n",
    "        sent_list = file.read().split('\\n')\n",
    "\n",
    "    for sentence in sent_list:\n",
    "        tokens = tokenize(sentence, tokenizer)\n",
    "        \n",
    "        counter.update(tokens)\n",
    "        \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "5fe245b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "# Load spacy tokenizer models, download them if they haven't been downloaded already\n",
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "id": "9102dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class Multi30k(Dataset):\n",
    "    \n",
    "    def __init__(self, en_list, de_list, en_tokenizer, de_tokenizer, en_vocab, de_vocab, max_seq_len):\n",
    "        \n",
    "        self.en_list = en_list\n",
    "        self.de_list = de_list\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.de_tokenizer = de_tokenizer\n",
    "        self.en_vocab = en_vocab\n",
    "        self.de_vocab = de_vocab\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        en_sent = self.en_list[idx]\n",
    "        de_sent = self.de_list[idx]\n",
    "        \n",
    "        en_tok = tokenize(en_sent, self.en_tokenizer)\n",
    "        de_tok = tokenize(de_sent, self.de_tokenizer)\n",
    "        \n",
    "        en_vect = []\n",
    "        de_vect = []\n",
    "        \n",
    "        en_vect.append(self.en_vocab('<start>'))\n",
    "        de_vect.append(self.de_vocab('<start>'))\n",
    "        en_vect.extend([self.en_vocab(token) for token in en_tok])\n",
    "        de_vect.extend([self.de_vocab(token) for token in de_tok])\n",
    "        \n",
    "        en_vect.append(self.en_vocab('<end>'))\n",
    "        de_vect.append(self.de_vocab('<end>'))\n",
    "        \n",
    "        max_seq = self.max_seq_len\n",
    "            \n",
    "        if len(en_vect) < max_seq:\n",
    "            tmp = [0] * (max_seq - len(en_vect))\n",
    "            en_vect.extend(tmp)\n",
    "            \n",
    "        if len(de_vect) < max_seq:\n",
    "            tmp = [0] * (max_seq - len(de_vect))\n",
    "            de_vect.extend(tmp)\n",
    "        \n",
    "        src = torch.tensor(en_vect, dtype=torch.long)\n",
    "        tgt = torch.tensor(de_vect, dtype=torch.long)\n",
    "        \n",
    "        return src, tgt\n",
    "    \n",
    "    def viewSentences(self, idx):\n",
    "    \n",
    "        en = self.en_list[idx]\n",
    "        de = self.de_list[idx]\n",
    "            \n",
    "        return en, de\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "id": "e29ab994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences(english_sentences, german_sentences, max_words):\n",
    "    filtered_english = []\n",
    "    filtered_german = []\n",
    "    sum_ = 0\n",
    "\n",
    "    for eng_sent, ger_sent in zip(english_sentences, german_sentences):\n",
    "        eng_words = len(eng_sent.split())\n",
    "        ger_words = len(ger_sent.split())\n",
    "        \n",
    "        # Subtracting two accounts for the start and stop tokens\n",
    "        if eng_words <= max_words-2 and ger_words <= max_words-2:\n",
    "            filtered_english.append(re.sub(r'[^\\w\\s]', '', eng_sent))\n",
    "            filtered_german.append(re.sub(r'[^\\w\\s]', '', ger_sent))\n",
    "\n",
    "    return filtered_english, filtered_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "id": "6ac3a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    src, tgt = zip(*data)\n",
    "    \n",
    "    src = torch.stack(src, 0)\n",
    "    tgt = torch.stack(tgt, 0)\n",
    "    labels = []\n",
    "    \n",
    "    for targ in tgt:\n",
    "        labels.append(targ[targ.nonzero().squeeze()])\n",
    "     \n",
    "    return src, tgt, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "87b7175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(en_list, de_list, en_tokenizer, de_tokenizer, en_vocab, de_vocab, max_seq_length, batch_size):\n",
    "    data = Multi30k(en_list, de_list, en_tokenizer, de_tokenizer, en_vocab, de_vocab, max_seq_length)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "id": "4aa9482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader_test = create_dataloader(filtered_en, filtered_de, spacy_en, spacy_de, en_vocab, de_vocab,\n",
    "                               max_seq_length=20, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "id": "bacc026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = data_loader_test[35]\n",
    "# a = a.unsqueeze(0)\n",
    "# b = b.unsqueeze(0)\n",
    "\n",
    "# s_enc, s_dec, c_dec = create_masks(a, b, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "id": "51f549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Sentence_Tokenization.ipynb\n",
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, de_batch, max_sequence_length):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "        try:\n",
    "            # Sometimes there's no padding\n",
    "            eng_end_idx = torch.where(eng_batch[idx] == 0)[0][0].item()\n",
    "        except:\n",
    "            eng_end_idx = max_sequence_length\n",
    "        try:\n",
    "            de_end_idx = torch.where(de_batch[idx] == 0)[0][0].item()\n",
    "        except:\n",
    "            de_end_idx = max_sequence_length\n",
    "        eng_chars_to_padding_mask = np.arange(eng_end_idx+1, max_sequence_length)\n",
    "        de_chars_to_padding_mask = np.arange(de_end_idx+1, max_sequence_length)\n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_self_attention[idx, :, de_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, de_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, de_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    #print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}:\\n {encoder_self_attention_mask[0, :10, :10]}\")\n",
    "    #print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}:\\n {decoder_self_attention_mask[0, :10, :10]}\")\n",
    "    #print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}:\\n {decoder_cross_attention_mask[0, :10, :10]}\")\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "0dab7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in dataset: 26945\n",
      "Parameters: 55522638\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and build vocabs if not done already\n",
    "spacy_de, spacy_en = load_tokenizers()\n",
    "\n",
    "# en_vocab = build_vocab(\"train.en\", spacy_en, threshold=2)\n",
    "# print(len(en_vocab))\n",
    "\n",
    "# de_vocab = build_vocab(\"train.de\", spacy_de, threshold=2)\n",
    "# print(len(de_vocab))\n",
    "\n",
    "# with open(\"en_vocab.pkl\", 'wb') as f:\n",
    "#     pickle.dump(en_vocab, f)\n",
    "    \n",
    "# with open(\"de_vocab.pkl\", 'wb') as f:\n",
    "#     pickle.dump(de_vocab, f)\n",
    "\n",
    "with open(\"./en_vocab.pkl\", 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"./de_vocab.pkl\", 'rb') as f:\n",
    "    de_vocab = pickle.load(f)\n",
    "\n",
    "# Define parameters\n",
    "heads = 8\n",
    "d_model = 512\n",
    "hidden = 2048\n",
    "max_sequence_length = 20\n",
    "num_layers = 6\n",
    "src_vocab_size = len(en_vocab)\n",
    "tgt_vocab_size = len(de_vocab)\n",
    "\n",
    "# Trim some sentences\n",
    "with io.open(\"train.en\", 'r', encoding='utf-8') as file:\n",
    "    en_list = file.read().split('\\n')\n",
    "    \n",
    "with io.open(\"train.de\", 'r', encoding='utf-8') as file:\n",
    "    de_list = file.read().split('\\n')\n",
    "    \n",
    "filtered_en, filtered_de = filter_sentences(en_list, de_list, max_words=max_sequence_length)\n",
    "\n",
    "print(\"Total sentences in dataset:\", len(filtered_en))\n",
    "\n",
    "data_loader = create_dataloader(filtered_en, filtered_de, spacy_en, spacy_de, en_vocab, de_vocab,\n",
    "                               max_seq_length=20, batch_size=2)\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=de_vocab.word2idx['<pad>'],\n",
    "                                reduction='none')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = Transformer(max_sequence_length=max_sequence_length,\n",
    "                    src_vocab_size=src_vocab_size,\n",
    "                    tgt_vocab_size=tgt_vocab_size,\n",
    "                    num_layers=num_layers,\n",
    "                    heads=heads,\n",
    "                    d_model=d_model,\n",
    "                    hidden=hidden)\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "\n",
    "for params in parameters:\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "# Total number of parameters\n",
    "print(\"Parameters:\",sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "033ce705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 20, 8014])\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(parameters, lr=0.0003)\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for i, (src, tgt, labels) in enumerate(data_loader):\n",
    "        # Create masks\n",
    "        enc_self_mask, dec_self_mask, dec_cross_mask = create_masks(src, tgt, max_sequence_length)\n",
    "        \n",
    "        logits = model(src, tgt, enc_self_mask, dec_self_mask, dec_cross_mask)\n",
    "        print(logits.shape)\n",
    "        break\n",
    "        #loss = torch.nn.functional.cross_entropy(logits, labels)\n",
    "        #model.zero_grad()\n",
    "        #loss.backward()\n",
    "        #opt.step()\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            print(loss.item())\n",
    "            #torch.save(decoder.state_dict(), './decoder_{}_{}.ckpt'.format(epoch, i))\n",
    "            #torch.save(encoder.state_dict(), './encoder_{}_{}.ckpt'.format(epoch, i))\n",
    "        break\n",
    "            \n",
    "# Save the model\n",
    "#torch.save(decoder.state_dict(), './decoder_final.ckpt')\n",
    "#torch.save(encoder.state_dict(), './encoder_final.ckpt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
