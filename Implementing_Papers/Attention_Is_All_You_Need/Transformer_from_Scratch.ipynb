{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421f8ea1",
   "metadata": {},
   "source": [
    "## Attention Is All You Need\n",
    "\n",
    "Original paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Implementing a transformer kind of from scratch using numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "29c5a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3f667",
   "metadata": {},
   "source": [
    "Need to implement:\n",
    "- [x] Scaled dot-product attention\n",
    "- [x] Multi-head attention\n",
    "- [x] Positional encoding\n",
    "- [x] Layer normalization\n",
    "- [x] Position-wise feed forward\n",
    "- [x] Embeddings\n",
    "- [x] Encoder layer (combination of some of the above)\n",
    "- [x] Encoder (stack of encoder layers)\n",
    "- [x] Multi-head cross attention\n",
    "- [x] Decoder layer\n",
    "- [x] Decoder\n",
    "- [x] Transformer (combining encoder and decoder, plus some additional stuff)\n",
    "- [ ] Weight init\n",
    "- [ ] Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ed01d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mask = torch.triu(torch.ones_like(x) * float('-inf'), diagonal=1)\n",
    "#mask = torch.triu(torch.ones(50,50) * float('-inf'), diagonal=1)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    numerator = q @ torch.transpose(k, -2, -1) # May have to fix this transpose\n",
    "    if mask is not None:\n",
    "        numerator = numerator + mask\n",
    "    denominator = math.sqrt(k.shape[-1])\n",
    "    attn = F.softmax((numerator/denominator), dim=-1, dtype=torch.float32)\n",
    "    result = attn @ v\n",
    "    return result, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fbac151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // heads # Embed dim must be divisible by heads\n",
    "        self.q_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.k_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.v_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.linear_out = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_length, _ = q.size()\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        q, k, v = [x.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1,2) for x in [q,k,v]]\n",
    "        values, attn = scaled_dot_product_attention(q, k, v, mask)\n",
    "        x = values.transpose(1,2).reshape(batch_size, seq_length, self.heads * self.head_dim)\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95b85eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn((30,50,512))\n",
    "\n",
    "mh = MultiHeadAttention(8, 512)\n",
    "res = mh(test, test, test)\n",
    "print(res.shape)\n",
    "\n",
    "# mh_torch = nn.MultiheadAttention(512, 8, bias=False, batch_first=True)\n",
    "# res1 = mh_torch(test, test, test)\n",
    "# print(res1[0].shape)\n",
    "\n",
    "# Check if tensors equal within threshold\n",
    "#torch.all(torch.lt(torch.abs(torch.add(res, -res1[0])), 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84ccc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example to visualize why you need this: x.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1,2)\n",
    "# as opposed to just reshaping to that desired shape only using view.\n",
    "ex_q = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "ex_k = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "ex_v = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "r = ex_q.view(2,3,5,6)\n",
    "t = ex_q.view(2,5,3,6).transpose(1,2)\n",
    "\n",
    "# Toy example: 2 batches with a sequence length of 5 and an embedding of size 18.\n",
    "# Keep in mind, ex_q is an example of what q would look like. If you print out ex_q, r, t. You can see that r simply\n",
    "# goes across row by row of ex_q dividing the data amongst the \"heads\" completely incorrectly as it's taking some info from \n",
    "# the first input sequence and then it carries over into the second input sequence, so it's clearly wrong which is why\n",
    "# you need to used both the view and transpose in order to move the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fe7ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30e046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, drop_prob=0.1): # Max seq length is set to 50\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Calculate denominator, it's the same for even and odd dimensions so you can reuse it\n",
    "        evens = torch.arange(0, self.d_model, 2).float()\n",
    "        denom = torch.pow(10000, evens/self.d_model)\n",
    "        \n",
    "        # Calculate positional encodings\n",
    "        self.pe = torch.zeros(self.max_seq_len, self.d_model)\n",
    "        positions = torch.arange(0, self.max_seq_len).float().reshape(self.max_seq_len, 1)\n",
    "        \n",
    "        self.pe[:, 0::2] = torch.sin(positions / denom)\n",
    "        self.pe[:, 1::2] = torch.cos(positions / denom)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17d3007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameter_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameter_shape = parameter_shape\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Define layer norm learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameter_shape))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # The layer norm is computed based on each matrix of the batch, not across the batch.\n",
    "        mean = inputs.mean(-1, keepdim=True)\n",
    "        std = inputs.std(-1, keepdim=True)\n",
    "        \n",
    "        norm = (self.gamma * ((inputs - mean) / (std + self.eps))) + self.beta\n",
    "        \n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a3db39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3ee2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x -> Multi-Head Attention -> LayerNorm(residual + x) -> PWFeedForward -> LayerNorm(residual + x)\n",
    "#\n",
    "# MultiHeadAttention: heads, d_model\n",
    "# LayerNormalization: parameter_shape, eps=1e-5\n",
    "# PositionWiseFeedForward: d_model, hidden, drop_prob=0.1\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm1 = LayerNormalization(self.d_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.pwff = PositionWiseFeedForward(self.d_model, self.hidden, self.drop_prob)\n",
    "        self.norm2 = LayerNormalization(self.d_model) # Might have to change this\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attn(x, x, x, mask=None)\n",
    "        x = self.norm1(residual_x + x)\n",
    "        x = self.drop1(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.pwff(x)\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfb9fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.mask_attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm1 = LayerNormalization(self.d_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.cross_attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm2 = LayerNormalization(self.d_model)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        self.pwff = PositionWiseFeedForward(self.d_model, self.hidden, self.drop_prob)\n",
    "        self.norm3 = LayerNormalization(self.d_model) # Might have to change this\n",
    "        self.drop3 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, y, mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.mask_attn(x, x, x, mask=mask)\n",
    "        x = self.norm1(residual_x + x)\n",
    "        x = self.drop1(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.cross_attn(x, y, y) # FINISH THIS\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.pwff(x)\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3d14c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x = inputs\n",
    "        for module in self._modules.values():\n",
    "            x = module(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0688ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            out = module(x, y, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad22ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(heads, d_model, hidden) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7521f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialEncoder(*[DecoderLayer(heads, d_model, hidden) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, y, mask):\n",
    "        x = self.layers(x, y, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf9779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_sequence_length, src_vocab_size, tgt_vocab_size,\n",
    "                 num_layers, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.src_embed = Embeddings(src_vocab_size, d_model)\n",
    "        self.tgt_embed = Embeddings(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.enc_pe = PositionalEncoding(d_model, max_sequence_length, drop_prob)\n",
    "        self.dec_pe = PositionalEncoding(d_model, max_sequence_length, drop_prob)\n",
    "        \n",
    "        self.encoder = Encoder(heads, d_model, hidden, num_layers)\n",
    "        self.decoder = Decoder(heads, d_model, hidden, num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, tgt, mask):\n",
    "        x = self.src_embed(src)\n",
    "        y = self.tgt_embed(tgt)\n",
    "        \n",
    "        x = self.enc_pe(x)\n",
    "        y = self.dec_pe(y)\n",
    "        \n",
    "        enc = self.encoder(x)\n",
    "        dec = self.decoder(y, enc, mask)\n",
    "        \n",
    "        out = self.linear(dec)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "024e9661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 44153866\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "heads = 8\n",
    "d_model = 512\n",
    "hidden = 2048\n",
    "max_sequence_length = 50\n",
    "num_layers = 6\n",
    "src_vocab_size = 10\n",
    "tgt_vocab_size = 10\n",
    "\n",
    "# Create Mask\n",
    "mask = torch.full([max_sequence_length, max_sequence_length] , float('-inf'))\n",
    "mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "model = Transformer(max_sequence_length=max_sequence_length,\n",
    "                    src_vocab_size=src_vocab_size,\n",
    "                    tgt_vocab_size=tgt_vocab_size,\n",
    "                    num_layers=num_layers,\n",
    "                    heads=heads,\n",
    "                    d_model=d_model,\n",
    "                    hidden=hidden)\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "\n",
    "# Total number of parameters\n",
    "print(\"Parameters:\",sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fb03420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import io\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(file, threshold=4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    with io.open(file, 'r', encoding='utf-8') as file:\n",
    "        sent_list = file.read().split('\\n')\n",
    "\n",
    "    for sentence in sent_list:\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence.lower())\n",
    "        \n",
    "        counter.update(tokens)\n",
    "        \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a435a964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# en_vocab = build_vocab(\"train.en\", threshold=2)\n",
    "# print(len(en_vocab))\n",
    "\n",
    "# de_vocab = build_vocab(\"train.de\", threshold=2)\n",
    "# print(len(de_vocab))\n",
    "\n",
    "# with open(\"en_vocab.pkl\", 'wb') as f:\n",
    "#     pickle.dump(en_vocab, f)\n",
    "    \n",
    "# with open(\"de_vocab.pkl\", 'wb') as f:\n",
    "#     pickle.dump(de_vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9102dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class Multi30k(Dataset):\n",
    "    \n",
    "    def __init__(self, en_list, de_list, src_vocab, tgt_vocab):\n",
    "        \n",
    "        self.en_list = en_list\n",
    "        self.de_list = de_list\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        en_sent = self.en_list[idx]\n",
    "        de_sent = self.de_list[idx]\n",
    "        \n",
    "        en_tok = nltk.tokenize.word_tokenize(en_sent)\n",
    "        de_tok = nltk.tokenize.word_tokenize(de_sent)\n",
    "        \n",
    "        en_vect = []\n",
    "        de_vect = []\n",
    "        \n",
    "        en_vect.append(vocab('<start>'))\n",
    "        de_vect.append(vocab('<start>'))\n",
    "        en_vect.extend([vocab(token) for token in tokens])\n",
    "        de_vect.extend([vocab(token) for token in tokens]) # change vocabs to correct version\n",
    "        en_vect.append(vocab('<end>'))\n",
    "        de_vect.append(vocab('<end>'))\n",
    "        \n",
    "        src = torch.tensor(en_vect, dtype=torch.long)\n",
    "        tgt = torch.tensor(de_vect, dtype=torch.long)\n",
    "        \n",
    "        return src, tgt\n",
    "    \n",
    "    def viewSentences(self, idx):\n",
    "    \n",
    "        en = self.en_list[idx]\n",
    "        de = self.de_list[idx]\n",
    "            \n",
    "        return en, de\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "25ef1cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    # Sort a data list by caption length (descending order).\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87b7175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(image_dir, dataframe, vocab, image_processor, batch_size=128):\n",
    "    flickr = FlickrDataset(image_dir, dataframe, vocab, image_processor)\n",
    "\n",
    "    # Data loader for COCO dataset\n",
    "    # This will return (images, captions, lengths) for each iteration.\n",
    "    # images: a tensor of shape (batch_size, 3, 224, 224).\n",
    "    # captions: a tensor of shape (batch_size, padded_length).\n",
    "    # lengths: a list indicating valid length for each caption. length is (batch_size).\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=flickr, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9e1c669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1435\n"
     ]
    }
   ],
   "source": [
    "with io.open(\"train.en\", 'r', encoding='utf-8') as file:\n",
    "    sent_list = file.read().split('\\n')\n",
    "    \n",
    "#longest_string = max(sent_list, key=len)\n",
    "lens = [len(s) for s in sent_list]\n",
    "count_greater = sum(1 for num in lens if num > 98)\n",
    "per = count_greater / len(lens)\n",
    "print(count_greater)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
