{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "421f8ea1",
   "metadata": {},
   "source": [
    "## Attention Is All You Need\n",
    "\n",
    "Original paper: https://arxiv.org/pdf/1706.03762.pdf\n",
    "\n",
    "Implementing a transformer kind of from scratch using numpy and torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c5a8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\thinc\\compat.py:36: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  hasattr(torch, \"has_mps\")\n",
      "C:\\Users\\Stephen\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\thinc\\compat.py:37: UserWarning: 'has_mps' is deprecated, please use 'torch.backends.mps.is_built()'\n",
      "  and torch.has_mps  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import spacy\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e3f667",
   "metadata": {},
   "source": [
    "Need to implement:\n",
    "- [x] Scaled dot-product attention\n",
    "- [x] Multi-head attention\n",
    "- [x] Positional encoding\n",
    "- [x] Layer normalization\n",
    "- [x] Position-wise feed forward\n",
    "- [x] Embeddings\n",
    "- [x] Encoder layer (combination of some of the above)\n",
    "- [x] Encoder (stack of encoder layers)\n",
    "- [x] Multi-head cross attention\n",
    "- [x] Decoder layer\n",
    "- [x] Decoder\n",
    "- [x] Transformer (combining encoder and decoder, plus some additional stuff)\n",
    "- [ ] Weight init\n",
    "- [ ] Optimization\n",
    "- [ ] Preprocess, Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ed01d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    numerator = q @ torch.transpose(k, -2, -1) # May have to fix this transpose\n",
    "    if mask is not None:\n",
    "        numerator = numerator.permute(1, 0, 2, 3) + mask\n",
    "        numerator = numerator.permute(1, 0, 2, 3)\n",
    "    denominator = math.sqrt(k.shape[-1])\n",
    "    attn = F.softmax((numerator/denominator), dim=-1, dtype=torch.float32)\n",
    "    result = attn @ v\n",
    "    return result, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbac151",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, heads, d_model):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.head_dim = d_model // heads # Embed dim must be divisible by heads\n",
    "        self.q_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.k_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.v_linear = nn.Linear(self.d_model, self.d_model)\n",
    "        self.linear_out = nn.Linear(self.d_model, self.d_model)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size, seq_length, _ = q.size()\n",
    "        q = self.q_linear(q)\n",
    "        k = self.k_linear(k)\n",
    "        v = self.v_linear(v)\n",
    "        q, k, v = [x.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1,2) for x in [q,k,v]]\n",
    "        values, attn = scaled_dot_product_attention(q, k, v, mask)\n",
    "        x = values.transpose(1,2).reshape(batch_size, seq_length, self.heads * self.head_dim)\n",
    "        x = self.linear_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95b85eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50, 512])\n"
     ]
    }
   ],
   "source": [
    "test = torch.randn((30,50,512))\n",
    "\n",
    "mh = MultiHeadAttention(8, 512)\n",
    "res = mh(test, test, test)\n",
    "print(res.shape)\n",
    "\n",
    "# mh_torch = nn.MultiheadAttention(512, 8, bias=False, batch_first=True)\n",
    "# res1 = mh_torch(test, test, test)\n",
    "# print(res1[0].shape)\n",
    "\n",
    "# Check if tensors equal within threshold\n",
    "#torch.all(torch.lt(torch.abs(torch.add(res, -res1[0])), 1e-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84ccc8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create example to visualize why you need this: x.view(batch_size, seq_length, self.heads, self.head_dim).transpose(1,2)\n",
    "# as opposed to just reshaping to that desired shape only using view.\n",
    "ex_q = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "ex_k = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "ex_v = torch.randint(low=0, high=10, size=(2,5,18))\n",
    "r = ex_q.view(2,3,5,6)\n",
    "t = ex_q.view(2,5,3,6).transpose(1,2)\n",
    "\n",
    "# Toy example: 2 batches with a sequence length of 5 and an embedding of size 18.\n",
    "# Keep in mind, ex_q is an example of what q would look like. If you print out ex_q, r, t. You can see that r simply\n",
    "# goes across row by row of ex_q dividing the data amongst the \"heads\" completely incorrectly as it's taking some info from \n",
    "# the first input sequence and then it carries over into the second input sequence, so it's clearly wrong which is why\n",
    "# you need to used both the view and transpose in order to move the data correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe7ba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "30e046dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len, drop_prob=0.1): # Max seq length is set to 50\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Calculate denominator, it's the same for even and odd dimensions so you can reuse it\n",
    "        evens = torch.arange(0, self.d_model, 2).float()\n",
    "        denom = torch.pow(10000, evens/self.d_model)\n",
    "        \n",
    "        # Calculate positional encodings\n",
    "        self.pe = torch.zeros(self.max_seq_len, self.d_model)\n",
    "        positions = torch.arange(0, self.max_seq_len).float().reshape(self.max_seq_len, 1)\n",
    "        \n",
    "        self.pe[:, 0::2] = torch.sin(positions / denom)\n",
    "        self.pe[:, 1::2] = torch.cos(positions / denom)\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17d3007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, parameter_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.parameter_shape = parameter_shape\n",
    "        self.eps = eps\n",
    "        \n",
    "        # Define layer norm learnable parameters\n",
    "        self.gamma = nn.Parameter(torch.ones(parameter_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(parameter_shape))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # The layer norm is computed based on each matrix of the batch, not across the batch.\n",
    "        mean = inputs.mean(-1, keepdim=True)\n",
    "        std = inputs.std(-1, keepdim=True)\n",
    "        \n",
    "        norm = (self.gamma * ((inputs - mean) / (std + self.eps))) + self.beta\n",
    "        \n",
    "        return norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a3db39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ee2e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x -> Multi-Head Attention -> LayerNorm(residual + x) -> PWFeedForward -> LayerNorm(residual + x)\n",
    "#\n",
    "# MultiHeadAttention: heads, d_model\n",
    "# LayerNormalization: parameter_shape, eps=1e-5\n",
    "# PositionWiseFeedForward: d_model, hidden, drop_prob=0.1\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm1 = LayerNormalization(self.d_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.pwff = PositionWiseFeedForward(self.d_model, self.hidden, self.drop_prob)\n",
    "        self.norm2 = LayerNormalization(self.d_model) # Might have to change this\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.attn(x, x, x, mask=mask)\n",
    "        x = self.norm1(residual_x + x)\n",
    "        x = self.drop1(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.pwff(x)\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfb9fb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.d_model = d_model\n",
    "        self.hidden = hidden\n",
    "        self.drop_prob = drop_prob\n",
    "        \n",
    "        self.mask_attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm1 = LayerNormalization(self.d_model)\n",
    "        self.drop1 = nn.Dropout(p=drop_prob)\n",
    "        self.cross_attn = MultiHeadAttention(self.heads, self.d_model)\n",
    "        self.norm2 = LayerNormalization(self.d_model)\n",
    "        self.drop2 = nn.Dropout(p=drop_prob)\n",
    "        self.pwff = PositionWiseFeedForward(self.d_model, self.hidden, self.drop_prob)\n",
    "        self.norm3 = LayerNormalization(self.d_model) # Might have to change this\n",
    "        self.drop3 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x, y, self_mask, cross_mask):\n",
    "        residual_x = x.clone()\n",
    "        x = self.mask_attn(x, x, x, mask=self_mask)\n",
    "        x = self.norm1(residual_x + x)\n",
    "        x = self.drop1(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.cross_attn(x, y, y, mask=cross_mask) # FINISH THIS \n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        residual_x = x.clone()\n",
    "        x = self.pwff(x)\n",
    "        x = self.norm2(residual_x + x)\n",
    "        x = self.drop2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d14c5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialEncoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            out = module(x, mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0688ab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self, *inputs):\n",
    "        x, y, self_mask, cross_mask = inputs\n",
    "        for module in self._modules.values():\n",
    "            out = module(x, y, self_mask, cross_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ad22ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialEncoder(*[EncoderLayer(heads, d_model, hidden) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        x = self.layers(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7521f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, heads, d_model, hidden, num_layers):\n",
    "        super().__init__()\n",
    "        self.layers = SequentialDecoder(*[DecoderLayer(heads, d_model, hidden) for _ in range(num_layers)])\n",
    "        \n",
    "    def forward(self, x, y, self_mask, cross_mask):\n",
    "        x = self.layers(x, y, self_mask, cross_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf9779d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, max_sequence_length, src_vocab_size, tgt_vocab_size,\n",
    "                 num_layers, heads, d_model, hidden, drop_prob=0.1):\n",
    "        super().__init__()\n",
    "        self.src_embed = Embeddings(src_vocab_size, d_model)\n",
    "        self.tgt_embed = Embeddings(tgt_vocab_size, d_model)\n",
    "        \n",
    "        self.enc_pe = PositionalEncoding(d_model, max_sequence_length, drop_prob)\n",
    "        self.dec_pe = PositionalEncoding(d_model, max_sequence_length, drop_prob)\n",
    "        \n",
    "        self.encoder = Encoder(heads, d_model, hidden, num_layers)\n",
    "        self.decoder = Decoder(heads, d_model, hidden, num_layers)\n",
    "        \n",
    "        self.linear = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, src, tgt, enc_self_mask, dec_self_mask, dec_cross_mask):\n",
    "        x = self.src_embed(src)\n",
    "        y = self.tgt_embed(tgt)\n",
    "        \n",
    "        x = self.enc_pe(x)\n",
    "        y = self.dec_pe(y)\n",
    "        \n",
    "        enc = self.encoder(x, enc_self_mask)\n",
    "        dec = self.decoder(y, enc, dec_self_mask, dec_cross_mask)\n",
    "        \n",
    "        out = self.linear(dec)\n",
    "        \n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2998e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/03-advanced/image_captioning\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import io\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)\n",
    "\n",
    "def build_vocab(file, tokenizer, threshold=4):\n",
    "    \"\"\"Build a simple vocabulary wrapper.\"\"\"\n",
    "    counter = Counter()\n",
    "    \n",
    "    with io.open(file, 'r', encoding='utf-8') as file:\n",
    "        sent_list = file.read().split('\\n')\n",
    "\n",
    "    for sentence in sent_list:\n",
    "        tokens = tokenize(sentence, tokenizer)\n",
    "        \n",
    "        counter.update(tokens)\n",
    "        \n",
    "    # If the word frequency is less than 'threshold', then the word is discarded.\n",
    "    words = [word for word, cnt in counter.items() if cnt >= threshold]\n",
    "\n",
    "    # Create a vocab wrapper and add some special tokens.\n",
    "    vocab = Vocabulary()\n",
    "    vocab.add_word('<pad>')\n",
    "    vocab.add_word('<start>')\n",
    "    vocab.add_word('<end>')\n",
    "    vocab.add_word('<unk>')\n",
    "\n",
    "    # Add the words to the vocabulary.\n",
    "    for i, word in enumerate(words):\n",
    "        vocab.add_word(word)\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5fe245b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From: https://nlp.seas.harvard.edu/annotated-transformer/\n",
    "# Load spacy tokenizer models, download them if they haven't been downloaded already\n",
    "def load_tokenizers():\n",
    "\n",
    "    try:\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download de_core_news_sm\")\n",
    "        spacy_de = spacy.load(\"de_core_news_sm\")\n",
    "\n",
    "    try:\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "    except IOError:\n",
    "        os.system(\"python -m spacy download en_core_web_sm\")\n",
    "        spacy_en = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    return spacy_de, spacy_en\n",
    "\n",
    "\n",
    "def tokenize(text, tokenizer):\n",
    "    return [tok.text for tok in tokenizer.tokenizer(text)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9102dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset class\n",
    "class Multi30k(Dataset):\n",
    "    \n",
    "    def __init__(self, en_list, de_list, en_tokenizer, de_tokenizer, en_vocab, de_vocab, max_seq_len):\n",
    "        \n",
    "        self.en_list = en_list\n",
    "        self.de_list = de_list\n",
    "        self.en_tokenizer = en_tokenizer\n",
    "        self.de_tokenizer = de_tokenizer\n",
    "        self.en_vocab = en_vocab\n",
    "        self.de_vocab = de_vocab\n",
    "        self.max_seq_len = max_seq_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        en_sent = self.en_list[idx]\n",
    "        de_sent = self.de_list[idx]\n",
    "        \n",
    "        en_tok = tokenize(en_sent, self.en_tokenizer)\n",
    "        de_tok = tokenize(de_sent, self.de_tokenizer)\n",
    "        \n",
    "        en_vect = []\n",
    "        de_vect = []\n",
    "        \n",
    "        en_vect.append(self.en_vocab('<start>'))\n",
    "        de_vect.append(self.de_vocab('<start>'))\n",
    "        en_vect.extend([self.en_vocab(token) for token in en_tok])\n",
    "        de_vect.extend([self.de_vocab(token) for token in de_tok])\n",
    "        \n",
    "        en_vect.append(self.en_vocab('<end>'))\n",
    "        de_vect.append(self.de_vocab('<end>'))\n",
    "        \n",
    "        max_seq = self.max_seq_len\n",
    "            \n",
    "        if len(en_vect) < max_seq:\n",
    "            tmp = [0] * (max_seq - len(en_vect))\n",
    "            en_vect.extend(tmp)\n",
    "            \n",
    "        if len(de_vect) < max_seq:\n",
    "            tmp = [0] * (max_seq - len(de_vect))\n",
    "            de_vect.extend(tmp)\n",
    "        \n",
    "        src = torch.tensor(en_vect, dtype=torch.long)\n",
    "        tgt = torch.tensor(de_vect, dtype=torch.long)\n",
    "        \n",
    "        return src, tgt\n",
    "    \n",
    "    def viewSentences(self, idx):\n",
    "    \n",
    "        en = self.en_list[idx]\n",
    "        de = self.de_list[idx]\n",
    "            \n",
    "        return en, de\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e29ab994",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences(english_sentences, german_sentences, max_words):\n",
    "    filtered_english = []\n",
    "    filtered_german = []\n",
    "    sum_ = 0\n",
    "\n",
    "    for eng_sent, ger_sent in zip(english_sentences, german_sentences):\n",
    "        eng_words = len(eng_sent.split())\n",
    "        ger_words = len(ger_sent.split())\n",
    "        \n",
    "        # Subtracting two accounts for the start and stop tokens\n",
    "        if eng_words <= max_words-2 and ger_words <= max_words-2:\n",
    "            filtered_english.append(re.sub(r'[^\\w\\s]', '', eng_sent))\n",
    "            filtered_german.append(re.sub(r'[^\\w\\s]', '', ger_sent))\n",
    "\n",
    "    return filtered_english, filtered_german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ac3a39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(data):\n",
    "    \n",
    "    src, tgt = zip(*data)\n",
    "    \n",
    "    src = torch.stack(src, 0)\n",
    "    tgt = torch.stack(tgt, 0)\n",
    "    labels = []\n",
    "    \n",
    "    for targ in tgt:\n",
    "        labels.append(targ[targ.nonzero().squeeze()])\n",
    "     \n",
    "    return src, tgt, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "87b7175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader(en_list, de_list, en_tokenizer, de_tokenizer, en_vocab, de_vocab, max_seq_length, batch_size):\n",
    "    data = Multi30k(en_list, de_list, en_tokenizer, de_tokenizer, en_vocab, de_vocab, max_seq_length)\n",
    "    data_loader = torch.utils.data.DataLoader(dataset=data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    return data_loader\n",
    "    #return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4aa9482d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtered_en' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m data_loader_test \u001b[38;5;241m=\u001b[39m create_dataloader(\u001b[43mfiltered_en\u001b[49m, filtered_de, spacy_en, spacy_de, en_vocab, de_vocab,\n\u001b[0;32m      2\u001b[0m                                max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'filtered_en' is not defined"
     ]
    }
   ],
   "source": [
    "data_loader_test = create_dataloader(filtered_en, filtered_de, spacy_en, spacy_de, en_vocab, de_vocab,\n",
    "                               max_seq_length=20, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bacc026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a, b = data_loader_test[35]\n",
    "# a = a.unsqueeze(0)\n",
    "# b = b.unsqueeze(0)\n",
    "\n",
    "# s_enc, s_dec, c_dec = create_masks(a, b, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "51f549b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: https://github.com/ajhalthor/Transformer-Neural-Network/blob/main/Sentence_Tokenization.ipynb\n",
    "NEG_INFTY = -1e9\n",
    "\n",
    "def create_masks(eng_batch, de_batch, max_sequence_length):\n",
    "    num_sentences = len(eng_batch)\n",
    "    look_ahead_mask = torch.full([max_sequence_length, max_sequence_length] , True)\n",
    "    look_ahead_mask = torch.triu(look_ahead_mask, diagonal=1)\n",
    "    encoder_padding_mask = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_self_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "    decoder_padding_mask_cross_attention = torch.full([num_sentences, max_sequence_length, max_sequence_length] , False)\n",
    "\n",
    "    for idx in range(num_sentences):\n",
    "        try:\n",
    "            # Sometimes there's no padding\n",
    "            eng_end_idx = torch.where(eng_batch[idx] == 0)[0][0].item()\n",
    "        except:\n",
    "            eng_end_idx = max_sequence_length\n",
    "        try:\n",
    "            de_end_idx = torch.where(de_batch[idx] == 0)[0][0].item()\n",
    "        except:\n",
    "            de_end_idx = max_sequence_length\n",
    "            \n",
    "        eng_chars_to_padding_mask = np.arange(eng_end_idx+1, max_sequence_length)\n",
    "        de_chars_to_padding_mask = np.arange(de_end_idx+1, max_sequence_length)\n",
    "        encoder_padding_mask[idx, :, eng_chars_to_padding_mask] = True\n",
    "        encoder_padding_mask[idx, eng_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_self_attention[idx, :, de_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_self_attention[idx, de_chars_to_padding_mask, :] = True\n",
    "        decoder_padding_mask_cross_attention[idx, :, eng_chars_to_padding_mask] = True\n",
    "        decoder_padding_mask_cross_attention[idx, de_chars_to_padding_mask, :] = True\n",
    "\n",
    "    encoder_self_attention_mask = torch.where(encoder_padding_mask, NEG_INFTY, 0)\n",
    "    decoder_self_attention_mask =  torch.where(look_ahead_mask + decoder_padding_mask_self_attention, NEG_INFTY, 0)\n",
    "    decoder_cross_attention_mask = torch.where(decoder_padding_mask_cross_attention, NEG_INFTY, 0)\n",
    "    #print(f\"encoder_self_attention_mask {encoder_self_attention_mask.size()}:\\n {encoder_self_attention_mask[0, :10, :10]}\")\n",
    "    #print(f\"decoder_self_attention_mask {decoder_self_attention_mask.size()}:\\n {decoder_self_attention_mask[0, :10, :10]}\")\n",
    "    #print(f\"decoder_cross_attention_mask {decoder_cross_attention_mask.size()}:\\n {decoder_cross_attention_mask[0, :10, :10]}\")\n",
    "    return encoder_self_attention_mask, decoder_self_attention_mask, decoder_cross_attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0dab7227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences in dataset: 26945\n",
      "Parameters: 8007710\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizers and build vocabs if not done already\n",
    "spacy_de, spacy_en = load_tokenizers()\n",
    "\n",
    "# en_vocab = build_vocab(\"train.en\", spacy_en, threshold=2)\n",
    "# print(len(en_vocab))\n",
    "\n",
    "# de_vocab = build_vocab(\"train.de\", spacy_de, threshold=2)\n",
    "# print(len(de_vocab))\n",
    "\n",
    "with open(\"./en_vocab.pkl\", 'rb') as f:\n",
    "    en_vocab = pickle.load(f)\n",
    "\n",
    "with open(\"./de_vocab.pkl\", 'rb') as f:\n",
    "    de_vocab = pickle.load(f)\n",
    "\n",
    "# Define parameters\n",
    "heads = 8\n",
    "d_model = 240#512\n",
    "hidden = 2048\n",
    "max_sequence_length = 20\n",
    "num_layers = 1#6\n",
    "src_vocab_size = len(en_vocab)\n",
    "tgt_vocab_size = len(de_vocab)\n",
    "\n",
    "# Trim some sentences\n",
    "with io.open(\"train.en\", 'r', encoding='utf-8') as file:\n",
    "    en_list = file.read().split('\\n')\n",
    "    \n",
    "with io.open(\"train.de\", 'r', encoding='utf-8') as file:\n",
    "    de_list = file.read().split('\\n')\n",
    "    \n",
    "filtered_en, filtered_de = filter_sentences(en_list, de_list, max_words=max_sequence_length)\n",
    "\n",
    "print(\"Total sentences in dataset:\", len(filtered_en))\n",
    "\n",
    "data_loader = create_dataloader(filtered_en, filtered_de, spacy_en, spacy_de, en_vocab, de_vocab,\n",
    "                               max_seq_length=max_sequence_length, batch_size=2)\n",
    "\n",
    "# When computing the loss, we are ignoring cases when the label is the padding token\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=de_vocab.word2idx['<pad>'],\n",
    "                                reduction='none')\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model = Transformer(max_sequence_length=max_sequence_length,\n",
    "                    src_vocab_size=src_vocab_size,\n",
    "                    tgt_vocab_size=tgt_vocab_size,\n",
    "                    num_layers=num_layers,\n",
    "                    heads=heads,\n",
    "                    d_model=d_model,\n",
    "                    hidden=hidden)\n",
    "\n",
    "parameters = list(model.parameters())\n",
    "\n",
    "for params in parameters:\n",
    "    if params.dim() > 1:\n",
    "        nn.init.xavier_uniform_(params)\n",
    "\n",
    "# Total number of parameters\n",
    "print(\"Parameters:\",sum(p.nelement() for p in parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "033ce705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.1838, 9.3030, 9.2869, 9.3258, 8.4496, 9.0553, 9.3912, 9.2222, 9.0135,\n",
      "        9.1257, 9.2356, 9.4772, 9.5953, 8.8611, 9.8692, 8.7445, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3377, 9.1465, 9.2013, 9.0884, 8.9458, 8.9641, 8.5061,\n",
      "        9.0782, 8.6354, 9.2082, 8.8919, 8.6776, 9.1810, 9.3513, 9.3635, 9.2638,\n",
      "        8.9157, 8.8610, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0890, 9.4051, 9.1622, 9.0489, 9.2666, 9.0582, 9.1294, 8.9840, 8.8595,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8314, 9.1910, 9.3888, 8.7623, 9.3059, 8.7719, 8.6368,\n",
      "        8.9671, 9.1683, 8.9220, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2605, 9.3449, 9.0096, 9.6747, 8.8460, 9.1411, 8.9451, 8.8977, 8.8350,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3229, 8.6614, 9.2399, 8.7257, 9.3328, 8.9703, 9.1210,\n",
      "        9.3184, 9.1305, 8.9464, 9.3028, 9.0831, 9.1147, 9.1676, 8.9411, 8.9050,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1519, 8.9107, 9.2028, 8.9145, 8.9670, 8.6752, 8.7394, 8.8934, 9.3417,\n",
      "        8.6998, 8.4278, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9239, 8.6836, 9.0465, 9.1732, 8.9868, 9.2122, 8.7784,\n",
      "        8.9650, 9.0121, 8.8887, 8.8116, 9.1060, 8.5991, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2518, 8.8385, 9.1197, 9.2019, 8.9011, 9.4185, 9.5067, 8.9995, 8.7602,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0539, 8.6885, 8.9787, 8.7166, 9.1868, 8.8655, 8.6030,\n",
      "        9.0016, 9.6721, 9.1466, 9.1075, 9.1345, 9.2950, 8.8613, 8.7600, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0521, 8.8620, 9.1488, 8.7748, 9.1488, 9.5211, 9.1115, 9.2089, 9.4984,\n",
      "        9.1368, 9.1823, 8.7619, 8.8728, 8.9304, 8.9937, 8.8171, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1751, 8.8213, 8.6834, 9.0034, 8.9807, 9.3320, 9.4374,\n",
      "        8.9644, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1584, 9.1426, 9.0788, 9.1129, 9.0521, 8.6499, 9.2951, 9.4310, 8.4910,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0951, 8.7429, 8.9196, 9.3062, 8.8868, 9.1595, 8.6729,\n",
      "        9.2670, 8.7524, 9.3798, 8.6239, 8.8976, 9.2903, 8.6902, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2542, 9.3785, 8.7306, 8.8314, 9.0761, 8.9478, 9.0664, 8.9846, 9.0357,\n",
      "        9.1010, 8.7912, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0976, 9.3822, 8.9346, 8.8977, 8.9104, 8.8549, 8.8516,\n",
      "        9.5208, 8.9517, 8.8598, 8.7434, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8822, 8.5910, 9.5958, 9.3736, 9.0206, 8.9017, 8.6849, 8.8090, 8.8348,\n",
      "        9.4382, 8.8065, 9.0502, 8.6218, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2576, 8.9481, 9.2205, 8.6072, 9.2179, 8.6745, 9.5846,\n",
      "        9.0586, 9.2122, 9.5464, 8.5017, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1076, 8.9038, 9.1562, 8.8378, 9.4090, 8.9790, 9.6136, 9.2316, 9.1025,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2290, 8.7943, 9.0794, 9.3147, 9.1498, 9.0073, 9.4164,\n",
      "        8.7044, 8.8861, 8.9689, 8.7514, 9.1271, 9.2814, 8.6679, 9.1477, 8.9303,\n",
      "        9.3050, 8.6914, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2978, 8.6989, 9.5684, 9.0137, 8.7187, 9.0914, 9.0285, 9.0362, 9.0032,\n",
      "        8.9842, 8.5316, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1702, 9.2971, 9.0906, 9.1867, 8.9689, 8.8026, 9.0341,\n",
      "        8.9287, 9.1072, 9.2772, 9.1666, 9.2600, 9.2869, 8.8294, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3230, 8.7951, 8.8120, 9.3460, 9.7042, 8.7266, 9.4079, 8.7285, 8.8337,\n",
      "        9.2852, 8.5106, 9.2659, 9.3681, 9.0714, 8.8417, 9.0453, 8.9188, 0.0000,\n",
      "        0.0000, 0.0000, 9.3495, 9.4285, 8.9372, 9.1006, 8.8693, 9.3096, 9.2485,\n",
      "        8.9188, 9.0776, 9.1863, 8.7912, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1207, 8.6330, 9.0923, 9.2200, 9.2567, 8.9953, 8.8480, 8.8619, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2180, 8.5297, 9.3128, 9.1666, 8.4177, 9.2145, 8.8793,\n",
      "        9.4215, 8.6951, 9.2101, 8.9256, 9.0942, 8.8001, 8.8425, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8982, 9.4141, 8.9996, 9.3868, 9.0059, 9.6864, 9.3849, 9.1909, 8.5725,\n",
      "        9.0233, 9.1377, 8.9035, 9.1211, 8.5773, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1281, 9.2655, 9.2183, 9.1630, 9.1266, 9.2110, 8.7245,\n",
      "        9.2742, 9.0655, 9.0249, 9.4448, 8.9177, 9.3167, 8.7765, 8.9157, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0904, 8.7168, 9.1135, 9.0650, 8.5722, 9.2675, 8.8112, 8.6901, 8.4852,\n",
      "        8.8211, 8.8983, 8.5106, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0981, 8.6065, 8.9909, 9.0336, 9.3303, 8.6936, 9.1815,\n",
      "        9.0452, 9.0853, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2582, 8.8271, 9.3646, 9.0396, 8.9401, 9.6270, 8.7725, 8.9012, 9.0714,\n",
      "        8.7420, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9584, 8.6884, 9.1476, 8.7874, 8.8469, 8.9187, 9.2152,\n",
      "        8.9817, 8.5492, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3070, 8.5829, 9.4887, 9.0630, 8.8206, 8.8546, 9.0898, 8.6328, 8.5180,\n",
      "        9.0218, 9.4567, 9.0731, 8.7393, 8.9883, 8.8003, 8.9474, 8.9512, 9.0455,\n",
      "        9.1397, 8.7702, 9.1795, 9.5459, 8.5955, 8.6694, 9.4562, 8.6560, 8.8084,\n",
      "        9.3985, 9.0169, 8.9060, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1290, 8.9002, 9.0448, 8.6347, 9.0258, 8.9793, 9.0852, 8.8626, 9.4206,\n",
      "        8.7492, 8.9355, 8.8914, 8.9175, 8.9645, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2582, 8.6792, 9.4676, 8.6534, 8.5818, 9.3209, 8.5885,\n",
      "        8.8723, 8.9817, 9.3659, 9.0722, 9.0690, 9.0365, 8.9869, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1804, 8.9418, 9.4222, 8.8392, 9.1081, 8.8523, 8.8415, 8.8743, 8.6708,\n",
      "        9.0580, 9.1687, 9.0019, 9.3542, 9.1935, 8.6490, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0439, 8.9938, 9.2858, 8.8558, 9.5432, 8.7492, 9.0450,\n",
      "        8.9301, 8.8789, 9.1817, 9.0512, 8.5566, 8.9612, 9.1269, 8.4936, 9.2919,\n",
      "        9.7001, 8.8736, 8.5760, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0860, 9.4812, 8.7327, 8.9874, 9.4219, 9.3388, 8.8727, 9.4133, 9.0454,\n",
      "        8.8942, 8.4460, 8.6490, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1591, 8.9123, 9.0558, 9.1802, 8.7085, 9.1974, 8.9703,\n",
      "        9.0243, 8.6419, 9.1257, 8.6269, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1383, 9.0011, 8.8816, 8.7751, 8.9040, 8.9851, 8.9509, 8.6919, 9.3147,\n",
      "        8.7930, 8.7040, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1670, 9.0267, 9.1284, 9.3074, 9.0189, 8.6275, 8.6636,\n",
      "        8.9314, 8.9565, 8.8345, 8.8245, 9.1067, 8.3092, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2156, 8.9605, 9.0443, 9.1806, 9.0781, 9.2901, 8.8793, 8.7253, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1046, 9.1052, 9.0892, 9.0603, 8.6910, 9.2230, 8.9537,\n",
      "        9.1905, 8.4835, 9.0578, 8.2090, 8.9685, 9.4619, 8.5032, 9.2933, 8.8870,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3876, 8.9877, 9.4944, 8.9069, 9.3332, 9.0851, 8.4821, 9.4181, 9.4321,\n",
      "        8.8646, 9.2765, 8.4169, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1986, 9.3111, 9.0172, 9.4594, 9.0263, 9.1953, 8.2659,\n",
      "        9.0497, 9.2599, 9.3436, 9.1658, 9.1883, 8.9287, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2406, 9.6050, 8.8604, 8.9166, 8.8215, 8.9689, 9.1633, 9.1069, 8.9106,\n",
      "        8.9986, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2288, 9.2110, 8.7764, 8.8622, 9.2619, 8.8887, 9.1283,\n",
      "        9.2898, 8.9435, 9.1647, 8.8702, 8.9038, 9.2050, 9.2862, 9.2661, 9.1380,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.8655, 9.3804, 8.5216, 8.6145, 8.9933, 9.0134, 9.3343, 8.6669, 8.9622,\n",
      "        9.1961, 8.8303, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0292, 9.2496, 9.1914, 8.7279, 8.8529, 9.2317, 8.4807,\n",
      "        9.0259, 8.8421, 8.8433, 8.5549, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.4437, 8.5972, 9.6934, 9.2752, 8.9925, 9.2993, 9.0313, 9.1058, 8.8005,\n",
      "        9.3706, 8.9999, 9.0445, 9.4547, 8.8700, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1368, 9.3472, 8.9807, 9.4305, 9.2115, 9.4650, 8.8516,\n",
      "        9.3045, 8.8053, 8.6401, 8.8371, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0287, 8.7422, 9.5694, 9.4668, 9.2761, 8.7413, 9.2927, 8.8489, 8.8790,\n",
      "        8.7824, 8.9289, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.7332, 9.3476, 9.1267, 8.6970, 9.1842, 8.4212, 9.5316,\n",
      "        9.2991, 9.5202, 9.2191, 8.7632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9808, 8.7957, 9.0707, 9.0515, 9.1925, 8.4701, 8.9649, 8.9782, 8.8733,\n",
      "        9.4791, 8.6836, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8692, 8.8416, 8.5639, 9.1281, 8.9642, 9.0897, 8.7812,\n",
      "        8.9195, 9.1275, 8.8625, 8.6277, 9.0342, 8.6748, 8.6074, 8.7898, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8900, 9.5045, 9.1486, 9.0270, 9.4025, 8.9957, 8.9523, 8.8799, 8.7317,\n",
      "        9.4871, 9.2361, 8.9738, 8.5979, 9.1066, 9.1562, 8.9833, 8.9441, 9.1333,\n",
      "        8.8888, 8.7225, 9.1115, 9.2194, 9.3718, 9.4529, 8.8712, 8.8511, 9.4229,\n",
      "        9.1104, 9.2023, 9.3425, 8.9508, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1159, 8.9766, 9.5461, 9.1323, 9.2019, 9.1196, 9.1043, 8.5442, 8.6057,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.5161, 8.8378, 8.9913, 8.8031, 8.7540, 8.9067, 8.8337,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3284, 9.4202, 8.9856, 9.1398, 8.6767, 9.0208, 9.3224, 8.7746, 9.0217,\n",
      "        8.6062, 8.9932, 8.7078, 9.0745, 9.0122, 8.5765, 9.4101, 9.0816, 0.0000,\n",
      "        0.0000, 0.0000, 8.9967, 9.3790, 9.0338, 8.8890, 8.5004, 9.1013, 9.5462,\n",
      "        8.7900, 9.3704, 8.6560, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8609, 8.8555, 9.1067, 8.7861, 8.7320, 9.4399, 9.1830, 8.8206, 8.9019,\n",
      "        9.3566, 9.1488, 8.4824, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1725, 9.0513, 8.8749, 9.1216, 8.9079, 9.2152, 8.9300,\n",
      "        8.7308, 8.6772, 8.7823, 8.9175, 8.8902, 9.3325, 9.5771, 8.8159, 8.8124,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1479, 8.9264, 8.9991, 9.6787, 9.2203, 9.4205, 9.4532, 9.2034, 9.1446,\n",
      "        8.7211, 8.6861, 8.5424, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9921, 9.1599, 9.2907, 8.3783, 9.4341, 8.9759, 9.1764,\n",
      "        9.1441, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8893, 9.3107, 8.8693, 8.9874, 8.9420, 9.1260, 8.9761, 8.8095, 9.3660,\n",
      "        8.8217, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1926, 8.7806, 9.0039, 8.8667, 9.2397, 9.0413, 9.2775,\n",
      "        9.7552, 8.9261, 8.7018, 8.7784, 9.2843, 8.7562, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1595, 8.6422, 9.0030, 9.3037, 8.4143, 8.9988, 8.8980, 8.8816, 8.5754,\n",
      "        8.6498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1172, 9.1632, 9.1798, 8.9539, 8.8464, 9.0352, 8.9094,\n",
      "        9.2159, 9.1863, 9.0822, 9.3545, 9.1028, 8.8028, 8.9997, 8.7538, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2547, 8.6128, 9.0773, 8.9920, 8.9944, 8.9493, 9.0425, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1235, 9.2642, 8.9657, 9.0749, 9.5342, 8.7611, 8.7396,\n",
      "        8.7801, 9.4559, 9.2494, 8.4669, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2270, 9.3760, 8.5637, 9.1261, 8.8052, 8.9727, 9.0356, 8.9060, 8.9518,\n",
      "        9.2474, 8.5630, 8.9308, 8.9529, 8.8582, 9.0738, 8.9015, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1707, 8.7435, 9.4889, 9.3108, 9.3625, 8.9895, 8.7902,\n",
      "        9.0155, 9.1659, 8.7891, 9.5300, 8.9272, 8.6051, 8.6336, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2947, 8.9660, 8.9749, 8.8959, 8.7465, 9.0630, 8.7868, 8.9310, 8.6425,\n",
      "        9.1074, 9.0265, 9.4408, 8.6916, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9072, 9.0211, 8.7497, 9.1923, 9.7279, 8.7726, 9.0631,\n",
      "        8.9748, 8.8645, 8.8648, 9.0099, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1090, 8.7229, 9.4182, 9.0329, 8.5896, 8.9971, 8.9658, 8.8489, 8.9469,\n",
      "        9.3481, 8.8534, 8.9904, 8.8826, 9.4183, 8.5338, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0029, 8.9065, 9.1694, 9.0668, 8.9182, 8.5737, 8.4878,\n",
      "        9.3001, 8.7044, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0154, 8.8089, 9.1465, 8.6753, 9.0612, 8.7950, 9.1961, 8.8902, 9.6083,\n",
      "        8.8620, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.5820, 9.6886, 9.0277, 9.1490, 9.1714, 9.3908, 8.9726,\n",
      "        8.8522, 8.6453, 8.8744, 9.1232, 8.5681, 8.6109, 8.7802, 8.7650, 8.9721,\n",
      "        8.7954, 8.9313, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1374, 8.9197, 8.5687, 9.2185, 9.2826, 8.9380, 8.7392, 9.0893, 8.3710,\n",
      "        9.3446, 9.2026, 8.4644, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9989, 8.8317, 8.9424, 8.9670, 8.9587, 8.9577, 9.0040,\n",
      "        9.5434, 9.0098, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2106, 8.5502, 9.1231, 8.8588, 9.1939, 8.8615, 9.2445, 8.8428, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2398, 9.3508, 8.7155, 8.8006, 9.2840, 9.2824, 8.7476,\n",
      "        9.1572, 8.7925, 9.0413, 8.7809, 8.6104, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1786, 9.0971, 9.2506, 9.1589, 8.7242, 9.4633, 9.1968, 8.8853, 8.6858,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0581, 9.4050, 9.1453, 9.0881, 8.6993, 8.9462, 9.0192,\n",
      "        8.8222, 8.5931, 8.5357, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9298, 8.9725, 9.0273, 9.0367, 8.7478, 9.0708, 8.6898, 9.0652, 8.8839,\n",
      "        8.9649, 8.8657, 8.7585, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1379, 8.4420, 9.3387, 8.7187, 8.7051, 9.1536, 8.8168,\n",
      "        9.1698, 9.1931, 8.9173, 8.7121, 8.9272, 8.7941, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0588, 8.7440, 8.8535, 9.5280, 8.6868, 8.8055, 9.0428, 8.7527, 9.0555,\n",
      "        9.3068, 9.1950, 9.5517, 9.3082, 8.6767, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9685, 9.0597, 9.6120, 8.9716, 8.7733, 8.4973, 8.6200,\n",
      "        8.7708, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2047, 9.0530, 8.7617, 9.0075, 8.7767, 8.8443, 8.8188, 9.6375, 8.9173,\n",
      "        8.7592, 9.1635, 9.0903, 8.8415, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3419, 8.7477, 8.7556, 8.9789, 8.8405, 9.1426, 9.6545,\n",
      "        9.4029, 9.4513, 9.3483, 8.5857, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3728, 8.7403, 9.4443, 9.2156, 8.8578, 9.1560, 9.1667, 8.8542, 9.3257,\n",
      "        8.8771, 8.9473, 9.2464, 9.3634, 8.8897, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9565, 8.7462, 9.0442, 9.1803, 9.2504, 8.6762, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.0512, 9.2152, 8.9603, 9.1558, 8.8168, 8.6864, 9.1007, 8.8309, 9.1589,\n",
      "        8.5682, 9.2918, 9.0089, 8.4273, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0932, 8.7345, 9.2154, 8.8196, 9.3384, 8.9574, 9.2230,\n",
      "        9.0763, 9.0452, 9.3917, 9.0920, 9.1086, 9.0254, 8.7158, 9.1348, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3132, 8.8534, 9.1736, 9.1212, 9.2977, 9.3969, 8.9651, 9.0989, 9.4684,\n",
      "        8.9063, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8181, 8.7931, 9.0048, 8.9056, 8.7925, 8.8629, 8.8083,\n",
      "        8.9006, 8.6741, 9.4158, 8.6368, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0181, 9.2640, 8.6080, 9.1978, 8.9960, 9.0942, 9.3046, 9.4558, 9.4610,\n",
      "        9.4810, 9.0849, 9.1652, 8.9076, 9.2955, 8.8021, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2282, 9.3233, 8.9105, 9.0294, 8.8520, 9.3817, 8.7108,\n",
      "        9.5173, 9.0429, 8.5382, 9.1011, 8.7377, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0860, 9.5284, 8.9084, 8.9443, 9.3710, 8.7286, 9.2351, 9.4792, 8.9872,\n",
      "        8.6453, 8.9803, 9.4363, 8.8319, 8.9675, 8.8369, 9.1252, 8.8352, 0.0000,\n",
      "        0.0000, 0.0000, 9.3039, 8.6293, 8.9060, 9.2056, 9.0121, 9.4099, 9.8449,\n",
      "        8.7863, 8.8620, 8.9424, 8.7479, 8.8222, 8.9409, 8.9576, 8.5788, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0541, 9.0572, 9.5057, 8.9741, 8.9587, 9.2883, 9.2289, 8.7905, 8.8424,\n",
      "        8.9663, 9.4205, 9.0273, 8.7602, 8.5617, 8.8921, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2588, 9.1913, 8.9432, 9.1455, 9.3624, 9.1652, 8.9424,\n",
      "        9.2149, 9.1757, 9.1597, 9.1933, 8.8118, 9.1076, 8.9071, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3251, 9.0385, 8.5609, 8.8705, 9.3208, 8.8953, 9.6871, 9.2338, 9.1477,\n",
      "        8.7076, 9.3729, 9.3712, 9.2698, 9.0497, 8.9287, 8.8678, 8.8832, 0.0000,\n",
      "        0.0000, 0.0000, 9.2175, 8.6749, 9.4610, 9.0984, 9.3179, 9.0863, 8.9293,\n",
      "        9.0526, 8.7688, 9.0948, 8.7061, 8.9980, 9.3616, 8.9660, 9.3555, 9.5012,\n",
      "        9.1281, 8.7900, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0158, 8.6931, 9.0406, 8.8834, 9.4100, 8.9360, 8.9028, 8.5639, 8.9374,\n",
      "        9.2701, 9.0155, 9.1878, 8.5046, 9.1813, 8.6961, 9.3673, 8.9619, 0.0000,\n",
      "        0.0000, 0.0000, 8.9822, 8.1829, 8.7495, 9.2832, 9.5049, 8.6504, 8.8130,\n",
      "        8.9330, 8.9044, 8.7191, 9.1957, 8.7678, 9.1009, 8.7370, 8.6585, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0858, 8.8850, 9.0840, 9.0990, 9.1848, 9.1337, 9.4187, 9.0155, 9.2144,\n",
      "        9.1435, 8.6086, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.4404, 8.8189, 9.2855, 9.1658, 8.9362, 9.2123, 9.1549,\n",
      "        8.6591, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1320, 8.5844, 9.0858, 8.9707, 8.7546, 9.5476, 9.1447, 8.7645, 8.9064,\n",
      "        8.6598, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2710, 9.2101, 8.8718, 8.8985, 9.2401, 8.9595, 9.4044,\n",
      "        9.0452, 8.8916, 8.9470, 8.9470, 9.2881, 8.6940, 8.7524, 8.7743, 8.4451,\n",
      "        8.8790, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1857, 8.5232, 9.6853, 9.1561, 9.5845, 9.0880, 9.1094, 9.5663, 8.8458,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9971, 8.8835, 9.0327, 9.0566, 8.4862, 8.8059, 9.2987,\n",
      "        8.8882, 9.4236, 9.0039, 9.0597, 8.8428, 8.9275, 8.9788, 8.9541, 8.9214,\n",
      "        8.7360, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0215, 8.8640, 9.1787, 8.9168, 8.9965, 8.8888, 8.9952, 9.1911, 9.0121,\n",
      "        8.7759, 9.0430, 8.4825, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9586, 9.5324, 8.5914, 8.9669, 8.7434, 9.3634, 8.6801,\n",
      "        8.9666, 8.7270, 9.4074, 9.3229, 9.0308, 9.0263, 8.7110, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3333, 8.8727, 8.6019, 9.6459, 9.3195, 9.0401, 8.9664, 9.5007, 9.1892,\n",
      "        8.6146, 9.0218, 8.9612, 8.8867, 8.8841, 9.0050, 8.8740, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0986, 9.2974, 8.7350, 9.2269, 8.9948, 9.5344, 9.0668,\n",
      "        9.2787, 8.9329, 8.6587, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9644, 8.9236, 9.2864, 9.1494, 9.0636, 9.5403, 9.2953, 9.0977, 9.4278,\n",
      "        9.3323, 8.7015, 9.0310, 8.8987, 9.0127, 9.3324, 9.2389, 8.8337, 0.0000,\n",
      "        0.0000, 0.0000, 9.1952, 8.6147, 9.5106, 9.2273, 9.1201, 8.7022, 9.1237,\n",
      "        8.8874, 8.9052, 8.9071, 9.1411, 8.5973, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9287, 8.6435, 9.1211, 9.1289, 8.8211, 9.2279, 8.8943, 8.8249, 8.8866,\n",
      "        8.9147, 8.6842, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.5702, 9.3006, 8.8388, 9.1827, 9.6052, 8.9029, 8.7672,\n",
      "        8.9497, 9.1425, 8.9249, 8.9271, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1080, 8.6883, 9.3407, 9.1117, 9.3913, 8.8633, 9.3176, 9.0574, 9.2785,\n",
      "        8.7106, 9.1003, 8.3299, 9.0444, 8.8766, 8.8195, 8.7390, 8.6934, 8.8485,\n",
      "        0.0000, 0.0000, 9.0852, 9.0722, 9.4519, 9.2705, 8.5347, 9.0380, 9.2997,\n",
      "        8.7809, 8.9130, 8.7512, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9796, 9.3839, 9.3010, 8.8251, 9.2485, 9.4985, 9.1699, 8.7899, 9.2807,\n",
      "        8.7083, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9413, 8.7706, 9.3328, 9.1125, 8.7189, 9.1973, 9.4129,\n",
      "        8.8314, 8.7886, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.7952, 9.4121, 9.0221, 9.2471, 8.8850, 9.7845, 8.6258, 8.9024, 8.8688,\n",
      "        8.8556, 8.9828, 8.8978, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0143, 8.9989, 9.1183, 9.1447, 9.0899, 9.0664, 8.5637,\n",
      "        9.2489, 9.1122, 8.6776, 9.3527, 8.5741, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0753, 8.9736, 9.5535, 9.1741, 8.7927, 9.1110, 9.1466, 9.2287, 8.7944,\n",
      "        9.0819, 8.8595, 8.8494, 9.3690, 9.0509, 9.0356, 8.8061, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2652, 8.6260, 8.6849, 8.9205, 9.2053, 8.9436, 8.9570,\n",
      "        8.9321, 8.7865, 9.5345, 9.1406, 9.0191, 8.7405, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0746, 8.8769, 8.9937, 8.9260, 9.0153, 9.0614, 8.8179, 9.0265, 8.8039,\n",
      "        9.5265, 8.7922, 8.9398, 8.5160, 9.0205, 8.5954, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9996, 9.1110, 9.4480, 8.5964, 8.8055, 9.4597, 8.7843,\n",
      "        9.3247, 8.8301, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0134, 9.2745, 8.9516, 8.7521, 8.9189, 8.7066, 9.7811, 8.8069, 9.3205,\n",
      "        8.9169, 9.0320, 9.1047, 8.8158, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1639, 8.8603, 9.2956, 8.7430, 8.5425, 8.8648, 8.6634,\n",
      "        9.1065, 8.7674, 8.6502, 8.8976, 8.4126, 9.1905, 8.7199, 8.7710, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2013, 9.2537, 9.2095, 9.1025, 8.4274, 9.0041, 8.9649, 9.1406, 8.8219,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0789, 8.9360, 8.6701, 9.4583, 9.5015, 8.9120, 9.5952,\n",
      "        8.8387, 8.8960, 9.0067, 8.7683, 9.1427, 9.2755, 8.9162, 8.8464, 8.9278,\n",
      "        8.9449, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0213, 9.2121, 8.7943, 9.0629, 9.0276, 8.9232, 9.5897, 9.2465, 9.5809,\n",
      "        9.2718, 8.4371, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0220, 8.8205, 8.7475, 8.7373, 9.0707, 8.6549, 8.6929,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2621, 8.7842, 9.3676, 9.1073, 8.8041, 8.9292, 8.7835, 9.5852, 9.2605,\n",
      "        8.6653, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2334, 8.7664, 8.7235, 9.5033, 8.5191, 9.0540, 8.9526,\n",
      "        9.1608, 8.7583, 8.9441, 8.4476, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2150, 9.1466, 9.0201, 9.1317, 9.0558, 9.0163, 9.0522, 8.8927, 9.3569,\n",
      "        8.7968, 8.8963, 9.1525, 8.4855, 8.9362, 9.0030, 8.9468, 8.9125, 0.0000,\n",
      "        0.0000, 0.0000, 9.1461, 9.3035, 8.8720, 9.3583, 9.0348, 8.7420, 8.8440,\n",
      "        8.9208, 8.8099, 8.7994, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1937, 9.3488, 8.5440, 9.0223, 8.9076, 9.0726, 9.0973, 9.3101, 8.8904,\n",
      "        9.0198, 9.4133, 9.3360, 9.0152, 8.8700, 9.2933, 9.2041, 8.9723, 8.9204,\n",
      "        0.0000, 0.0000, 8.9957, 8.9579, 9.4158, 9.1183, 8.9427, 9.1850, 8.2958,\n",
      "        9.2461, 8.7930, 8.7417, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2322, 8.5701, 9.0616, 8.7406, 8.9833, 8.8021, 8.8812, 8.7801, 8.9003,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3179, 8.8124, 9.3306, 9.2554, 8.9909, 9.2072, 8.9620,\n",
      "        8.9330, 8.9644, 8.6632, 8.8307, 8.7357, 9.2965, 9.2872, 9.1297, 8.8418,\n",
      "        9.1551, 8.8283, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9315, 9.2853, 9.1610, 8.8906, 8.8781, 9.2141, 8.8678, 8.9089, 9.0587,\n",
      "        8.6624, 8.7354, 8.5483, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.7935, 9.0215, 9.4134, 9.2152, 9.1144, 9.0158, 8.8046,\n",
      "        9.0093, 8.8365, 9.4860, 8.8141, 8.4437, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1229, 9.1938, 9.0510, 9.3713, 9.1259, 8.8496, 9.1767, 9.2320, 8.6890,\n",
      "        9.0915, 8.7072, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1942, 8.9956, 9.3176, 8.7365, 9.0593, 8.7990, 8.9567,\n",
      "        8.7945, 8.8384, 8.7801, 8.9345, 9.1324, 8.5961, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1324, 9.1214, 9.1102, 9.3102, 8.3933, 8.9606, 8.6302, 9.0065, 8.4949,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8955, 9.1560, 9.0146, 8.6393, 8.9749, 9.3777, 9.2434,\n",
      "        8.6538, 8.8785, 9.0990, 8.9884, 8.4690, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9727, 8.7560, 8.8667, 8.9220, 9.3732, 9.6598, 8.9868, 9.1983, 9.0115,\n",
      "        8.8194, 8.9839, 8.9661, 9.3498, 9.1189, 8.6364, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3576, 9.3497, 8.9881, 9.0028, 9.2819, 9.0076, 9.4015,\n",
      "        8.7269, 9.0177, 8.7488, 8.5879, 8.9091, 9.3366, 9.0032, 9.1636, 8.8336,\n",
      "        9.3351, 8.8605, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1851, 9.1523, 9.0697, 8.7070, 9.5047, 9.3674, 8.8735, 9.7013, 8.7384,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1815, 8.7275, 9.4287, 9.0611, 9.3349, 8.5766, 9.3402,\n",
      "        8.9006, 9.5500, 8.8149, 8.8048, 8.6276, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0620, 9.0342, 9.1583, 9.1433, 8.9341, 8.8886, 8.7234, 8.2603, 8.8991,\n",
      "        8.8703, 8.8724, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3911, 8.7326, 9.1669, 8.9197, 8.6751, 9.3249, 8.6031,\n",
      "        9.4332, 8.9759, 9.0588, 8.8316, 9.2438, 9.3764, 9.2526, 8.6942, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.0050, 9.5351, 9.0406, 8.7038, 9.2058, 9.1914, 8.7267, 9.3306, 9.3288,\n",
      "        8.8799, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2695, 8.4946, 9.7367, 8.5050, 8.9322, 8.9048, 8.6884,\n",
      "        8.9801, 9.4244, 8.7635, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1600, 8.7034, 8.9239, 8.8080, 8.4964, 8.5164, 9.2558, 8.3187, 8.9097,\n",
      "        9.1900, 9.1411, 8.9647, 8.8074, 8.6245, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8805, 9.0032, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2105, 9.4639, 8.8974, 9.2128, 8.9678, 9.0083, 9.4749, 8.8674, 9.2386,\n",
      "        8.9522, 8.4467, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8487, 9.2065, 9.1741, 8.8266, 8.9671, 9.3423, 8.7885,\n",
      "        9.1377, 8.7468, 9.0783, 9.1096, 8.9670, 8.5028, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0830, 8.9500, 8.7116, 8.8857, 8.7890, 8.7393, 9.3076, 9.5072, 9.2094,\n",
      "        9.1046, 8.7156, 8.3667, 9.1123, 9.0979, 9.0533, 8.7100, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2154, 8.6848, 9.2224, 8.9786, 9.3119, 8.8579, 9.0780,\n",
      "        9.1486, 8.4844, 9.0953, 8.8238, 9.2397, 8.9372, 8.6717, 8.8559, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1725, 8.9100, 9.0601, 9.1681, 9.0231, 8.5328, 9.0677, 8.7780, 9.3019,\n",
      "        8.5652, 9.0331, 9.2977, 8.7496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1607, 9.5579, 9.3750, 8.9586, 9.1822, 8.5104, 9.0468,\n",
      "        9.0044, 9.1710, 9.2960, 8.8768, 9.0546, 8.9155, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0983, 8.9036, 9.3546, 9.4657, 9.1072, 9.0444, 9.0716, 9.1512, 9.0499,\n",
      "        8.9076, 8.7402, 8.9855, 8.7796, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0540, 8.6310, 8.5468, 9.3475, 9.5261, 8.9682, 8.5014,\n",
      "        8.8503, 8.4604, 8.9025, 8.9082, 9.2180, 9.1145, 8.8469, 9.0227, 8.7704,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8885, 8.6337, 9.2784, 9.3898, 9.0139, 9.4135, 8.9631, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2784, 8.9493, 8.9042, 9.2795, 8.8199, 8.8614, 8.7393,\n",
      "        9.4848, 8.9848, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1914, 8.7136, 9.3652, 9.1224, 8.8469, 9.3183, 8.8821, 9.0368, 9.0178,\n",
      "        8.7998, 9.1083, 9.2033, 8.9826, 9.1816, 8.8805, 8.6681, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9996, 8.6594, 9.4934, 9.1483, 9.0566, 9.3362, 9.1081,\n",
      "        8.7978, 8.7726, 8.6965, 8.5977, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2104, 8.9637, 9.1198, 8.6861, 8.9038, 8.6589, 9.2950, 9.1161, 8.9286,\n",
      "        9.0742, 9.2312, 9.1860, 8.8089, 9.0838, 9.2762, 8.8722, 8.9055, 0.0000,\n",
      "        0.0000, 0.0000, 9.2790, 8.8240, 9.3193, 9.4481, 9.0552, 9.1505, 8.6113,\n",
      "        8.9314, 9.0535, 8.5573, 9.2152, 9.3350, 8.6959, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9616, 8.5717, 9.2168, 9.2534, 9.2593, 9.3231, 8.9165, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3702, 8.9472, 9.0670, 8.5774, 8.9875, 8.7739, 8.8111,\n",
      "        9.1352, 8.9908, 8.7229, 9.0188, 9.4084, 9.2298, 8.9892, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1910, 8.9833, 8.8070, 8.9134, 8.9963, 9.1412, 9.4761, 9.1567, 9.0378,\n",
      "        8.6649, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0454, 8.9060, 8.8736, 8.7315, 8.9628, 8.5397, 9.1240,\n",
      "        9.2419, 8.7274, 8.9001, 8.9054, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0884, 8.7353, 9.4246, 8.9850, 9.1540, 9.2333, 9.7846, 9.0537, 9.0221,\n",
      "        9.5243, 9.1864, 9.0308, 8.6002, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0606, 8.7274, 8.8547, 8.8928, 8.9862, 8.6179, 9.3097,\n",
      "        8.6319, 8.8669, 9.2824, 8.9915, 9.6259, 8.6749, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0114, 8.9685, 9.2685, 8.7006, 8.9711, 9.0583, 8.9142, 8.8255, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1060, 8.6379, 9.0396, 9.4787, 8.6824, 9.3549, 8.7266,\n",
      "        8.6726, 8.6081, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3836, 9.4479, 9.0342, 9.0022, 9.1513, 8.9454, 9.5048, 9.3562, 8.9152,\n",
      "        9.3537, 8.7985, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.7914, 9.1399, 9.0214, 9.1177, 8.9342, 9.5327, 9.3089,\n",
      "        9.1120, 9.3300, 8.8527, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9905, 9.0920, 8.9080, 8.8464, 8.8240, 8.9686, 8.9480, 8.8283, 8.3284,\n",
      "        8.8101, 9.4401, 8.6636, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.7963, 8.6045, 9.0247, 9.2604, 9.0861, 8.9476, 9.5204,\n",
      "        8.3383, 9.2697, 8.9635, 8.7057, 9.0526, 9.2147, 8.7382, 9.0341, 8.7507,\n",
      "        8.9228, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9985, 8.7310, 9.0115, 8.7994, 9.1249, 9.0109, 9.2980, 8.9929, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0493, 9.0097, 8.5163, 9.2209, 8.7450, 9.1512, 9.5666,\n",
      "        8.7394, 9.0412, 9.0081, 9.5170, 9.1386, 9.1787, 8.7144, 9.1813, 9.2705,\n",
      "        8.5415, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9641, 8.8069, 9.5526, 8.6985, 8.9836, 9.6137, 8.9665, 9.3925, 8.9963,\n",
      "        8.5436, 9.0462, 9.2734, 9.1443, 8.3339, 9.0469, 9.1789, 8.8645, 8.5437,\n",
      "        0.0000, 0.0000, 9.1022, 9.4604, 9.0007, 8.7709, 8.8566, 8.9912, 9.3841,\n",
      "        8.9991, 9.0334, 8.8435, 9.0496, 8.6146, 9.0880, 9.0034, 9.3405, 9.2910,\n",
      "        8.8823, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0814, 8.5504, 8.7378, 9.0171, 9.0972, 9.2609, 9.1266, 9.6138, 9.4072,\n",
      "        8.7186, 9.0096, 9.1561, 9.1633, 8.8462, 9.0835, 8.8807, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0309, 8.7153, 9.4197, 9.3163, 9.4501, 8.9552, 9.0345,\n",
      "        9.2388, 8.8982, 8.7546, 9.0505, 8.5474, 9.0416, 8.7133, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3661, 9.2513, 9.1222, 9.5258, 9.2017, 9.0154, 9.0421, 8.8125, 8.7744,\n",
      "        9.3873, 8.7733, 8.6502, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3319, 8.8185, 9.0426, 9.0832, 8.9358, 9.1239, 8.9023,\n",
      "        9.1743, 8.5611, 9.0230, 8.7471, 9.0576, 8.6372, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2438, 8.8640, 9.1701, 9.0470, 8.6312, 9.5912, 8.7572, 9.3271, 8.8451,\n",
      "        8.8785, 9.2562, 8.9445, 9.4442, 8.9798, 8.9362, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2662, 9.3192, 9.0668, 9.4527, 9.1272, 8.4534, 8.8905,\n",
      "        9.1722, 8.6503, 9.2330, 9.2635, 8.7995, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8033, 9.3227, 8.9298, 9.0351, 9.2146, 9.4774, 8.8690, 8.7564, 8.7618,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9216, 8.8961, 9.5010, 9.1263, 8.2806, 8.9952, 9.1395,\n",
      "        8.6195, 8.8533, 9.4768, 8.9970, 9.2648, 8.8610, 9.2782, 9.1763, 8.9523,\n",
      "        8.8299, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9486, 8.9162, 8.7820, 9.1902, 8.8110, 9.0294, 8.9288, 9.2769, 9.3292,\n",
      "        9.3568, 8.8573, 9.5790, 9.2573, 9.0084, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2080, 9.2510, 8.8907, 9.0817, 9.4211, 8.8779, 9.2329,\n",
      "        9.7475, 8.6148, 9.4507, 9.2938, 8.7142, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2718, 8.9286, 9.3505, 9.2721, 9.0688, 9.0286, 9.0033, 9.2763, 8.6275,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1744, 8.8664, 9.2911, 9.2094, 8.4882, 9.2333, 8.6883,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9508, 8.5566, 9.3820, 9.0317, 9.1904, 8.7994, 8.6076, 8.7255, 8.9737,\n",
      "        9.0517, 8.9289, 8.7501, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0457, 8.7322, 8.7843, 9.0193, 9.0512, 8.9453, 8.9003,\n",
      "        8.5862, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9935, 8.6387, 9.3278, 9.6097, 8.9814, 9.3628, 9.0490, 8.7139, 9.4804,\n",
      "        9.5410, 9.2354, 8.9666, 9.0110, 8.8585, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1627, 9.2896, 9.1102, 9.1140, 8.8209, 9.6085, 8.8002,\n",
      "        8.5123, 8.9974, 9.0881, 9.0552, 9.1156, 8.4950, 9.0840, 8.4904, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0244, 8.7388, 8.9540, 9.1221, 9.1222, 8.7806, 9.0877, 8.7288, 9.0151,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9041, 9.5010, 9.1606, 8.8045, 9.4087, 9.0147, 8.9205,\n",
      "        9.0737, 8.8876, 8.8239, 9.0764, 8.9004, 8.6513, 8.7740, 9.1121, 9.3514,\n",
      "        8.6923, 8.6674, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.3145, 9.4237, 8.8799, 8.9998, 8.8825, 8.8960, 9.3513, 9.2820, 9.1282,\n",
      "        9.1480, 8.3230, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2336, 9.4977, 8.7429, 8.6604, 8.8328, 9.0587, 9.5618,\n",
      "        9.0573, 8.9960, 9.1572, 9.0497, 9.0693, 8.6077, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1909, 9.2587, 9.3751, 8.9733, 9.6324, 8.9079, 9.3075, 8.7586, 8.9515,\n",
      "        8.7919, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2198, 9.4129, 8.9956, 8.6905, 8.6223, 9.0786, 8.5810,\n",
      "        8.8935, 9.2446, 8.8168, 9.0520, 9.3879, 8.7365, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0983, 8.8916, 8.9756, 9.0142, 8.9792, 9.2973, 8.8374, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9350, 8.8957, 9.3559, 8.7704, 9.3848, 8.7530, 8.7176,\n",
      "        9.0597, 9.1043, 8.8495, 9.0426, 8.5761, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1373, 8.5874, 8.7215, 9.1796, 8.8932, 9.1925, 9.1262, 8.7798, 8.8639,\n",
      "        9.1572, 9.0006, 8.9826, 9.1339, 8.9329, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2915, 8.7935, 9.1399, 8.8594, 8.7923, 8.9155, 9.0691,\n",
      "        8.4220, 8.9964, 8.7492, 9.1026, 8.7489, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0465, 9.0549, 8.7281, 9.3160, 8.8783, 8.6043, 9.3300, 9.1908, 9.3891,\n",
      "        8.8679, 8.8632, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1872, 9.3819, 8.9220, 8.7949, 9.1916, 8.9925, 8.9994,\n",
      "        9.4769, 9.1440, 8.8915, 8.4731, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9126, 9.2819, 8.5685, 8.6688, 8.8463, 9.3092, 9.1420, 9.2625, 8.7400,\n",
      "        8.4868, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0924, 9.3447, 8.8964, 9.0574, 8.9360, 9.5263, 9.1937,\n",
      "        9.1569, 8.9917, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1694, 9.0192, 9.3646, 8.8461, 9.0942, 8.8729, 8.7302, 9.1176, 8.8233,\n",
      "        8.8663, 9.2256, 9.0652, 9.2195, 9.0577, 9.3066, 8.8998, 8.8024, 8.8426,\n",
      "        0.0000, 0.0000, 9.3154, 9.1645, 9.3714, 8.8623, 8.6045, 8.6801, 8.9026,\n",
      "        8.8266, 8.8169, 8.7030, 8.8769, 9.0988, 8.5935, 9.0692, 8.5628, 8.7730,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3121, 9.2038, 9.0885, 8.9557, 9.3386, 8.8314, 9.1741, 8.9132, 8.8149,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8689, 9.2822, 9.2815, 9.2967, 9.1199, 8.8715, 9.2736,\n",
      "        9.0091, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1590, 8.9105, 9.3157, 8.9135, 9.0492, 9.1098, 9.2390, 8.7533, 8.7412,\n",
      "        9.0415, 8.9408, 9.1395, 8.8835, 9.3692, 9.2143, 8.6578, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1831, 8.7403, 8.9683, 8.9800, 8.8360, 8.8789, 9.4521,\n",
      "        8.6048, 8.9129, 9.2793, 8.5891, 8.9527, 8.8193, 9.0363, 8.9969, 8.9545,\n",
      "        8.8009, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1173, 8.6325, 9.6694, 9.1406, 9.3800, 9.2106, 9.1809, 8.8715, 9.1179,\n",
      "        9.1901, 9.2590, 9.3394, 8.9175, 8.5639, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1371, 9.0613, 9.1576, 9.1877, 9.0509, 8.9496, 9.5441,\n",
      "        8.6399, 9.2829, 8.9577, 8.6786, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2595, 9.5604, 9.0267, 9.0199, 9.0482, 9.1291, 9.0159, 9.0517, 9.1017,\n",
      "        9.3153, 8.7510, 9.2416, 8.9854, 8.8885, 8.2735, 8.9454, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0802, 9.5423, 9.1338, 8.9157, 8.7098, 9.2941, 9.1294,\n",
      "        9.0217, 8.8278, 9.0273, 8.6436, 9.6736, 8.5526, 8.5126, 8.9086, 8.9651,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9660, 9.4108, 8.5445, 8.8179, 8.9249, 9.1043, 9.2914, 9.1169, 9.0919,\n",
      "        8.8803, 9.1735, 9.3393, 9.2173, 8.7751, 9.3065, 9.0601, 8.9448, 0.0000,\n",
      "        0.0000, 0.0000, 9.3728, 9.3433, 9.3417, 8.7786, 8.9713, 8.7930, 9.2700,\n",
      "        9.4030, 9.0947, 8.6177, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.4753, 8.9248, 9.4122, 9.0217, 9.3271, 9.4438, 9.2680, 8.7306, 8.9336,\n",
      "        9.1208, 9.0743, 9.1323, 9.0664, 8.7526, 9.1475, 8.7964, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0645, 9.1481, 8.9624, 9.0114, 9.2101, 8.5575, 8.9766,\n",
      "        9.4210, 9.1212, 8.8384, 9.1992, 9.1417, 9.2320, 8.5798, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2620, 9.4455, 9.0704, 9.3865, 8.5915, 8.4178, 9.2897, 8.6063, 8.8612,\n",
      "        8.7537, 8.7344, 8.6833, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0310, 8.9455, 9.3111, 9.4417, 8.7749, 8.7925, 9.4535,\n",
      "        8.9789, 9.3009, 8.8424, 8.8886, 8.9776, 8.9169, 9.1158, 9.0395, 9.5153,\n",
      "        8.6807, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9820, 9.2625, 9.0241, 9.5580, 9.2473, 8.8273, 8.6258, 8.6176, 8.7770,\n",
      "        8.8177, 9.2147, 9.2311, 9.1662, 8.7669, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0742, 8.6531, 8.8408, 9.2697, 9.3635, 9.3132, 9.0440,\n",
      "        8.7595, 8.5406, 9.0798, 9.5424, 9.4711, 9.6006, 9.0201, 8.9066, 9.1088,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1833, 8.8904, 8.8309, 9.4192, 9.3846, 8.9039, 8.6945, 9.3718, 8.9845,\n",
      "        8.5349, 8.7817, 9.2794, 8.8581, 9.0521, 9.1303, 8.6532, 8.6582, 9.5744,\n",
      "        8.7899, 0.0000, 9.1048, 8.7552, 9.1260, 9.1068, 8.9047, 9.4207, 9.0999,\n",
      "        9.1757, 8.8821, 8.7887, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2557, 8.8748, 9.2247, 8.9658, 9.0474, 8.7318, 9.7567, 9.1928, 9.3083,\n",
      "        8.5913, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9662, 9.0679, 8.7994, 8.9911, 9.1395, 8.6451, 9.2527,\n",
      "        8.9327, 8.9946, 8.6089, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1417, 8.7532, 9.1569, 9.1035, 8.7085, 8.5578, 9.0213, 8.7549, 8.5537,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9815, 9.1183, 9.1360, 8.9794, 9.0064, 9.1656, 8.8446,\n",
      "        8.9423, 9.1748, 9.1333, 8.7007, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1037, 9.0868, 8.9785, 8.2512, 9.0942, 9.1142, 9.0744, 8.5977, 9.4792,\n",
      "        8.8949, 8.3989, 9.1712, 8.8787, 8.9568, 9.3559, 8.7594, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2865, 9.0243, 9.3291, 9.1679, 9.4002, 8.6538, 8.9942,\n",
      "        8.6554, 9.2764, 8.8500, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2996, 9.2351, 9.0891, 8.6259, 8.9226, 9.1665, 9.5322, 9.4036, 9.0620,\n",
      "        8.9043, 8.7584, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1174, 8.6683, 9.3236, 9.3075, 8.8586, 8.8622, 9.4413,\n",
      "        9.1328, 8.6384, 8.8100, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3494, 9.0431, 9.4209, 9.2087, 8.9348, 9.4842, 8.7092, 9.1475, 8.9150,\n",
      "        9.1270, 9.3398, 9.3961, 8.7765, 9.1979, 8.4537, 8.8075, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0622, 9.2325, 9.0283, 9.1348, 9.4606, 8.7581, 9.0602,\n",
      "        8.9303, 8.7591, 8.9474, 9.0255, 8.5736, 8.8324, 8.5358, 8.7928, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2966, 9.1230, 8.9346, 9.1636, 8.9889, 9.2232, 8.9877, 9.0892, 8.6731,\n",
      "        8.6984, 9.2024, 9.0241, 8.7995, 9.0262, 9.1414, 8.3949, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9718, 8.5490, 8.7346, 9.6854, 9.2721, 8.9066, 8.6595,\n",
      "        9.1473, 9.0149, 8.8935, 8.7631, 8.7215, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8305, 8.9145, 9.5858, 8.8145, 8.8297, 8.8310, 8.9126, 9.4848, 8.7705,\n",
      "        9.1712, 7.9619, 8.9861, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2467, 8.6483, 9.1581, 8.6531, 9.1599, 9.0300, 9.0623,\n",
      "        8.6442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9.0549, 8.6233, 8.5719, 9.2400, 9.3141, 9.0309, 9.4492, 9.1127, 9.3769,\n",
      "        8.6833, 8.8264, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3939, 9.3931, 8.8693, 8.9955, 9.1851, 9.3247, 9.4267,\n",
      "        9.1569, 8.8260, 9.3553, 8.7973, 8.8744, 8.8936, 9.0795, 8.7113, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1426, 9.0392, 8.2080, 9.2634, 9.0506, 8.9194, 8.9741, 8.9580, 8.9990,\n",
      "        9.3108, 8.4969, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2610, 8.9481, 9.1211, 9.0737, 9.3852, 9.0518, 8.7602,\n",
      "        9.0137, 8.9567, 8.9707, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1310, 8.6738, 8.8573, 9.1456, 9.0147, 8.5282, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1169, 9.0727, 8.9125, 8.7541, 9.0115, 9.0971, 9.4740,\n",
      "        9.0167, 9.0544, 8.8222, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0187, 9.2057, 8.7509, 8.7435, 8.9691, 9.1083, 8.6467, 9.1812, 9.4473,\n",
      "        8.7134, 8.8062, 8.8749, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0228, 8.7639, 9.3802, 9.2783, 8.9408, 9.2759, 9.1047,\n",
      "        9.1693, 8.5599, 8.7905, 9.4538, 8.7159, 9.3515, 9.0503, 9.0787, 8.6923,\n",
      "        8.6355, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.7439, 9.5389, 8.8671, 9.3853, 9.0423, 9.0418, 9.0218, 9.0849, 9.3400,\n",
      "        8.7906, 9.1392, 8.8311, 9.1375, 9.1365, 8.9246, 8.7678, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1124, 9.1997, 8.9864, 9.4991, 9.0866, 9.1969, 9.1157,\n",
      "        8.8698, 8.9383, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0418, 8.7276, 9.4560, 8.7832, 9.3208, 8.7028, 9.1657, 8.8205, 8.9620,\n",
      "        9.0151, 8.7001, 8.7400, 8.6466, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9910, 8.7882, 9.2208, 8.7807, 9.3194, 8.9945, 8.6975,\n",
      "        8.7772, 8.8560, 9.0594, 8.6036, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1393, 9.4934, 8.6101, 8.8431, 9.0884, 8.7473, 8.9847, 9.2662, 9.1476,\n",
      "        9.4857, 8.6784, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2131, 9.1955, 9.2328, 9.4219, 9.2523, 9.1395, 8.7747,\n",
      "        9.0910, 8.7071, 9.4864, 8.8881, 9.1063, 8.9846, 8.9897, 8.7618, 8.9364,\n",
      "        8.9554, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3575, 8.6202, 8.9682, 9.5377, 9.5583, 9.3340, 8.8432, 8.8788, 9.2095,\n",
      "        9.5503, 8.8327, 9.0692, 9.0814, 8.9351, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2165, 8.8462, 9.0524, 8.7558, 9.3171, 8.9571, 9.4282,\n",
      "        8.9548, 9.1375, 8.8766, 9.2080, 8.6757, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0364, 8.9602, 8.7123, 9.2168, 9.0886, 9.2528, 8.6079, 9.4169, 8.7737,\n",
      "        8.7359, 9.1204, 8.5092, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2628, 9.0732, 9.5982, 9.5650, 8.7917, 9.0970, 8.5768,\n",
      "        8.8611, 9.4155, 8.9433, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2492, 8.4165, 9.1384, 8.9459, 9.2113, 9.5480, 8.9866, 9.0031, 9.1692,\n",
      "        8.6654, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1983, 9.4161, 9.0603, 8.8094, 9.6316, 8.8491, 9.1040,\n",
      "        8.9227, 9.1963, 8.9730, 8.9669, 9.2907, 8.7697, 9.0693, 9.2272, 8.7598,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0903, 9.2757, 8.9133, 8.1694, 9.0838, 8.8567, 9.1531, 8.7850, 9.2395,\n",
      "        9.0858, 8.8877, 8.3642, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0117, 8.7234, 9.0948, 8.9324, 8.9580, 9.0524, 9.1790,\n",
      "        8.9232, 9.1677, 9.1253, 8.8264, 8.8615, 8.9292, 8.6069, 8.9949, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1346, 9.5969, 8.9978, 9.0884, 9.3109, 8.9498, 9.0868, 9.2001, 9.2930,\n",
      "        8.6653, 8.8446, 9.2392, 9.3104, 9.1268, 8.6367, 8.9327, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.5654, 8.9955, 9.2572, 9.0171, 9.2101, 9.1173, 9.2566,\n",
      "        8.5442, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.4094, 9.2835, 9.0921, 9.1870, 8.9917, 8.8737, 8.7035, 9.0667, 8.6759,\n",
      "        9.2317, 9.1142, 8.8699, 8.4633, 9.2961, 8.7634, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.4671, 9.3078, 9.5240, 8.8468, 9.3343, 8.8240, 8.7474,\n",
      "        9.0210, 9.2544, 8.6493, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2831, 9.5726, 8.7249, 9.4885, 8.8945, 9.0166, 9.3528, 8.6488, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1901, 8.8855, 9.1328, 8.6575, 8.9845, 9.0867, 9.0511,\n",
      "        9.1895, 9.3733, 8.4182, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0368, 8.8924, 9.1064, 9.3209, 8.6750, 9.4285, 9.2412, 8.7342, 9.2829,\n",
      "        8.4839, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.4270, 8.6223, 9.4980, 9.4353, 8.9177, 9.0144, 8.5370,\n",
      "        8.9679, 9.0201, 8.8425, 8.7379, 9.1590, 8.6728, 8.8169, 8.7076, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2774, 8.7551, 9.0739, 9.0484, 9.1453, 9.1038, 9.2371, 8.8924, 9.3118,\n",
      "        8.9344, 9.1474, 9.3998, 9.0121, 8.8596, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2029, 8.7038, 9.1248, 9.0054, 9.1248, 9.1854, 8.9675,\n",
      "        9.2313, 9.3088, 8.7545, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0970, 8.8144, 9.8466, 8.5690, 9.4447, 8.7100, 8.8938, 8.7347, 8.8464,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2503, 9.2100, 8.8078, 8.8212, 8.9877, 9.1763, 8.8487,\n",
      "        9.1249, 8.8175, 8.6554, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8189, 8.9921, 9.1630, 9.0806, 9.1396, 8.8536, 8.7423, 8.7398, 9.3854,\n",
      "        8.8464, 8.4725, 8.6962, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0574, 9.1947, 8.7885, 9.2198, 9.3031, 9.4554, 8.8878,\n",
      "        8.9657, 9.0960, 8.9850, 9.1004, 8.5963, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1967, 8.9196, 9.4024, 8.9377, 9.6175, 9.0816, 8.7442, 9.1248, 9.1961,\n",
      "        8.9814, 9.2190, 9.2409, 9.2639, 9.3143, 8.9126, 9.0817, 9.2253, 8.7830,\n",
      "        0.0000, 0.0000, 9.2033, 8.9831, 9.1544, 9.1024, 8.9273, 8.8326, 8.5073,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2202, 8.5401, 9.2079, 9.2062, 8.8709, 8.6896, 8.9388, 8.9536, 8.5901,\n",
      "        9.4650, 8.6834, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3288, 8.8100, 9.2924, 8.9768, 9.2248, 9.1028, 9.1031,\n",
      "        9.0991, 8.9447, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9843, 8.8766, 9.3984, 9.3634, 9.0422, 8.6892, 9.3439, 8.9418, 8.9459,\n",
      "        9.0468, 9.0722, 8.6657, 9.1119, 8.9262, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1344, 9.1341, 8.7444, 8.7668, 8.6251, 8.6570, 8.5361,\n",
      "        9.2964, 8.9327, 8.8997, 9.0315, 8.8965, 9.2943, 8.7446, 8.6994, 8.6002,\n",
      "        8.8895, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0250, 9.1682, 9.5061, 8.8449, 8.7936, 9.0785, 8.8854, 8.7674, 8.8744,\n",
      "        8.7842, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1580, 8.8329, 9.4710, 8.6473, 9.8025, 8.8534, 9.4453,\n",
      "        8.2003, 8.7990, 9.2911, 8.8494, 9.2417, 8.7467, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0626, 8.5693, 8.9821, 9.1788, 9.1254, 9.5373, 8.8828, 9.2615, 9.1256,\n",
      "        8.7169, 9.3676, 8.6739, 8.7323, 9.0260, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3212, 8.6020, 8.7066, 9.0992, 8.7762, 8.7261, 8.7996,\n",
      "        9.1734, 9.3923, 8.4944, 9.3291, 8.6525, 8.4857, 9.1310, 8.8919, 8.7229,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.9888, 8.9087, 9.1772, 8.7602, 8.9075, 9.5739, 8.7630, 9.2520, 9.0584,\n",
      "        8.9741, 8.6150, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9881, 8.8761, 8.4339, 9.0760, 9.2088, 9.6362, 9.1084,\n",
      "        9.0406, 8.9693, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9205, 8.6803, 8.6748, 9.1607, 8.8659, 9.3698, 8.7203, 9.0819, 8.8867,\n",
      "        8.7153, 9.1785, 8.6860, 9.4438, 8.8796, 8.7626, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2727, 8.7768, 9.3147, 8.6097, 9.4001, 9.2968, 9.3510,\n",
      "        8.7388, 8.7944, 8.6872, 9.1881, 8.7779, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8153, 8.9479, 9.4621, 8.8865, 9.1283, 8.6281, 9.1076, 8.6133, 8.8328,\n",
      "        8.7142, 8.9983, 9.2269, 9.0635, 8.8302, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8626, 9.3508, 8.9144, 8.5722, 9.2647, 9.3643, 8.9589,\n",
      "        9.3147, 8.9565, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9491, 9.5619, 9.0816, 8.7450, 9.4496, 9.5012, 9.3794, 9.1696, 8.6400,\n",
      "        9.1792, 8.6674, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1530, 9.2902, 8.8343, 9.2383, 9.2465, 8.8638, 9.2687,\n",
      "        9.2386, 9.2683, 9.2173, 9.2072, 8.7951, 8.6503, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2013, 9.1183, 8.8880, 8.8865, 9.0478, 8.4937, 8.8696, 8.5854, 9.1686,\n",
      "        8.6720, 8.7963, 8.7436, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.4336, 8.5397, 8.6314, 9.0090, 9.3559, 9.3825, 9.2181,\n",
      "        8.9730, 9.1192, 9.3606, 9.2214, 9.2373, 8.8334, 9.5204, 9.0948, 8.9640,\n",
      "        9.1786, 8.4997, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3079, 9.1503, 8.8871, 9.0716, 8.6313, 9.1961, 9.1372, 9.1862, 9.3885,\n",
      "        9.0918, 8.7182, 9.1401, 9.2918, 8.9605, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0924, 8.9817, 9.3945, 9.0067, 8.8185, 8.5289, 8.7671,\n",
      "        8.7005, 8.8070, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1528, 8.4290, 9.5753, 9.5117, 8.7553, 8.9722, 9.1979, 9.3922, 9.1667,\n",
      "        9.1796, 8.7654, 9.2827, 8.7328, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1492, 8.8823, 8.8445, 9.0070, 9.5633, 8.7961, 9.2398,\n",
      "        8.8815, 8.9631, 8.9434, 8.9765, 9.0977, 8.8505, 8.8203, 9.0896, 8.8545,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0193, 9.0263, 9.1653, 8.5238, 9.1595, 8.9858, 8.7515, 8.7775, 9.0367,\n",
      "        8.8383, 9.1684, 8.6498, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1999, 9.4551, 8.8591, 9.0375, 8.8104, 9.2416, 8.3315,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0356, 8.9650, 9.2932, 8.9466, 9.3286, 9.1776, 9.4081, 8.7192, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0601, 9.4103, 9.0796, 8.7519, 9.1675, 9.5125, 9.2233,\n",
      "        8.9633, 8.7662, 8.3496, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1896, 8.7268, 8.6417, 8.8891, 9.4487, 8.8491, 9.3288, 9.4075, 8.6982,\n",
      "        8.8867, 9.1375, 8.9134, 8.5148, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9193, 9.4810, 8.7229, 8.7748, 9.1874, 9.2603, 8.5558,\n",
      "        8.8196, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2037, 8.7650, 8.6228, 9.2160, 8.9530, 9.3348, 9.0926, 9.0453, 8.7284,\n",
      "        9.1142, 9.2428, 8.9344, 9.2251, 9.2456, 9.4862, 8.7650, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8811, 8.6255, 9.3069, 8.8943, 8.9464, 9.1587, 8.8551,\n",
      "        8.6820, 8.8946, 8.9795, 8.6321, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.3069, 8.7320, 9.0275, 9.3574, 8.7409, 9.1172, 9.1453, 8.8318, 9.3557,\n",
      "        8.9435, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8707, 9.0677, 8.9889, 9.1532, 9.0265, 8.5279, 8.9140,\n",
      "        8.8775, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0878, 9.0071, 8.9129, 8.8780, 8.7403, 8.9099, 8.8713, 8.9787, 9.1333,\n",
      "        9.5341, 8.8621, 9.2564, 8.9345, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2174, 8.7676, 9.2733, 8.6629, 8.9413, 8.9622, 9.1201,\n",
      "        8.7328, 8.7604, 9.8202, 9.0626, 9.2046, 9.3074, 8.7060, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2345, 9.0240, 9.2369, 8.9997, 8.9059, 9.1155, 9.2071, 9.3659, 9.0578,\n",
      "        8.8862, 9.4743, 9.5350, 9.5198, 9.0724, 9.3408, 9.0555, 9.2655, 9.2059,\n",
      "        8.7044, 0.0000, 9.0840, 8.8743, 9.2366, 9.6294, 8.5702, 8.8782, 8.5452,\n",
      "        8.8009, 8.8643, 8.7243, 9.3450, 9.0416, 8.5950, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.5670, 8.8385, 8.9850, 9.4277, 9.1880, 9.5280, 9.2224, 8.5383, 8.9318,\n",
      "        8.7827, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.9432, 8.8564, 9.2325, 8.9043, 9.0177, 9.1007, 9.6530,\n",
      "        8.9419, 9.0013, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1291, 8.6130, 8.8028, 8.6383, 8.6256, 9.4881, 9.0072, 8.8700, 8.5819,\n",
      "        9.3535, 8.8796, 8.6034, 8.9123, 9.2743, 9.0589, 8.8311, 8.9342, 8.8190,\n",
      "        0.0000, 0.0000, 9.0598, 9.1825, 8.8407, 9.2461, 8.7212, 9.3774, 9.1298,\n",
      "        8.5665, 8.9630, 8.7397, 8.6351, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.1182, 8.7612, 9.7917, 8.9703, 8.9696, 9.1513, 8.9110, 9.0297, 8.7833,\n",
      "        8.9419, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.1766, 8.8720, 8.7144, 9.3623, 9.3452, 8.5719, 8.9996,\n",
      "        8.8873, 8.9327, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.8019, 9.1016, 9.5193, 9.5948, 8.5424, 8.8668, 8.6361, 9.1442, 9.3180,\n",
      "        8.7753, 9.6560, 8.7088, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2055, 9.1320, 8.8677, 8.9629, 9.0369, 9.1116, 9.4670,\n",
      "        8.4682, 8.7590, 8.7580, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2700, 9.1995, 8.8420, 8.6564, 9.3156, 9.0265, 8.7839, 9.0810, 9.2748,\n",
      "        8.6295, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.4492, 9.2295, 9.0708, 8.8404, 8.9618, 9.1862, 9.7150,\n",
      "        9.0573, 9.2109, 8.6603, 9.1247, 9.2772, 8.6130, 8.4955, 8.7906, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.2090, 9.5239, 9.0119, 9.1260, 9.0821, 9.2457, 8.6823, 8.9176, 8.8539,\n",
      "        9.1433, 8.6259, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.2588, 8.7431, 9.1835, 9.0292, 8.9909, 8.5200, 9.6046,\n",
      "        9.0976, 8.9143, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([8.9083, 9.2440, 8.9087, 9.1810, 9.0131, 9.1815, 8.6131, 8.8195, 8.8659,\n",
      "        8.5989, 9.1580, 9.1871, 8.5641, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.3664, 8.7643, 8.9487, 9.4268, 9.0218, 8.7009, 9.4305,\n",
      "        9.1655, 9.1729, 9.0603, 9.1044, 9.1907, 9.3448, 8.3122, 9.0246, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0817, 9.3831, 8.5098, 9.6475, 8.9655, 8.7248, 9.0253, 8.8109, 8.8306,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 8.8897, 9.3251, 9.3212, 9.0374, 9.0532, 9.1211, 9.2859,\n",
      "        8.6527, 9.0111, 8.8541, 8.6801, 9.0956, 8.5177, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n",
      "tensor([9.0773, 9.4383, 9.0151, 9.3236, 8.7132, 9.0427, 9.2709, 8.6037, 9.1025,\n",
      "        8.8858, 9.3976, 9.0975, 9.3665, 8.7860, 8.9496, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 9.0311, 8.8973, 9.5179, 8.9316, 8.7010, 9.1665, 9.1713,\n",
      "        8.8057, 9.1663, 8.7946, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "        0.0000, 0.0000, 0.0000, 0.0000], grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (src, tgt, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_loader):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# Create masks\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     enc_self_mask, dec_self_mask, dec_cross_mask \u001b[38;5;241m=\u001b[39m create_masks(src, tgt, max_sequence_length)\n\u001b[1;32m---> 10\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_self_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_self_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_cross_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     logits \u001b[38;5;241m=\u001b[39m logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,tgt_vocab_size)\n\u001b[0;32m     12\u001b[0m     label \u001b[38;5;241m=\u001b[39m tgt\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 24\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, src, tgt, enc_self_mask, dec_self_mask, dec_cross_mask)\u001b[0m\n\u001b[0;32m     21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_pe(x)\n\u001b[0;32m     22\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_pe(y)\n\u001b[1;32m---> 24\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc_self_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(y, enc, dec_self_mask, dec_cross_mask)\n\u001b[0;32m     27\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear(dec)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, mask):\n\u001b[1;32m----> 7\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[12], line 5\u001b[0m, in \u001b[0;36mSequentialEncoder.forward\u001b[1;34m(self, *inputs)\u001b[0m\n\u001b[0;32m      3\u001b[0m x, mask \u001b[38;5;241m=\u001b[39m inputs\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m----> 5\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m, in \u001b[0;36mEncoderLayer.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpwff(x)\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(residual_x \u001b[38;5;241m+\u001b[39m x)\n\u001b[1;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\PyTorchEnv\\lib\\site-packages\\torch\\nn\\functional.py:1266\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1266\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(parameters, lr=0.0003)\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 1\n",
    "for epoch in range(epochs):\n",
    "    for i, (src, tgt, labels) in enumerate(data_loader):\n",
    "        # Create masks\n",
    "        enc_self_mask, dec_self_mask, dec_cross_mask = create_masks(src, tgt, max_sequence_length)\n",
    "        logits = model(src, tgt, enc_self_mask, dec_self_mask, dec_cross_mask)\n",
    "        logits = logits.view(-1,tgt_vocab_size)\n",
    "        label = tgt.view(-1)\n",
    "        loss = criterion(logits, label)\n",
    "        print(loss)\n",
    "        #model.zero_grad()\n",
    "        #loss.backward()\n",
    "        #opt.step()\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            continue\n",
    "            #print(loss.item())\n",
    "            #torch.save(decoder.state_dict(), './decoder_{}_{}.ckpt'.format(epoch, i))\n",
    "            #torch.save(encoder.state_dict(), './encoder_{}_{}.ckpt'.format(epoch, i))\n",
    "            \n",
    "# Save the model\n",
    "#torch.save(decoder.state_dict(), './decoder_final.ckpt')\n",
    "#torch.save(encoder.state_dict(), './encoder_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de060209",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
